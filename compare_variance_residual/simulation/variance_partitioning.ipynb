{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T21:46:47.768352Z",
     "start_time": "2025-03-20T21:46:47.563341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import himalaya.scoring\n",
    "import matplotlib.pyplot as plt\n",
    "import simplstyles\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from himalaya.backend import set_backend, get_backend\n",
    "from himalaya.ridge import RidgeCV, GroupRidgeCV\n",
    "\n",
    "from dataset import generate_dataset"
   ],
   "id": "861ec8f5e6880f5e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T21:47:34.237527Z",
     "start_time": "2025-03-20T21:47:32.696333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "set_backend('torch_cuda', on_error='warn')\n",
    "backend = get_backend()\n",
    "plt.style.use('nord-light-talk')"
   ],
   "id": "919bb24dd5993049",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T21:46:47.785770Z",
     "start_time": "2025-03-20T21:46:47.781896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_list = [100, 100, 100]\n",
    "# scalars = [1 / 3, 1 / 2, 1 / 6]\n",
    "scalars = [1 / 3, 1 / 3, 1 / 3]\n",
    "n_targets = 10000\n",
    "n_samples_train = 10000\n",
    "n_samples_test = 100\n",
    "n_samples = n_samples_train + n_samples_test\n",
    "noise_scalar = 0.1"
   ],
   "id": "545232a15bee20c4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T21:46:47.791710Z",
     "start_time": "2025-03-20T21:46:47.787673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "alphas = np.logspace(-5, 5, 10)\n",
    "cv = 5\n",
    "score_func = himalaya.scoring.r2_score"
   ],
   "id": "2a5592f16b917748",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T21:47:18.875878Z",
     "start_time": "2025-03-20T21:46:47.793729Z"
    }
   },
   "cell_type": "code",
   "source": "Xs, Y = generate_dataset(d_list, scalars, n_targets, n_samples, noise_scalar)",
   "id": "fb5340fddafa7afc",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "train joint model",
   "id": "1cfa268734d494b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T21:47:18.882711Z",
     "start_time": "2025-03-20T21:47:18.878087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "banded_solver_params = dict(n_iter=5, alphas=alphas, warn=False, score_func=score_func, n_targets_batch=1000)\n",
    "joint_model = GroupRidgeCV(groups=\"input\", solver_params=banded_solver_params)"
   ],
   "id": "2035066da3048a1b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T21:47:47.816724Z",
     "start_time": "2025-03-20T21:47:37.395412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "joint_model.fit([x[:n_samples_train] for x in Xs], Y[:n_samples_train])\n",
    "joint_score = joint_model.score([x[n_samples_train:] for x in Xs], Y[n_samples_train:])\n",
    "joint_score = backend.to_numpy(joint_score)"
   ],
   "id": "51c6ca7d508c1da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[........................................] 100% | 9.75 sec | 5 random sampling with cv | \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.displot(joint_score)\n",
    "plt.axvline(joint_score.mean(), label=\"mean\", color=\"C1\")\n",
    "plt.text(joint_score.mean(), 0, f\"{joint_score.mean():.2f}\", ha='center', va='bottom')"
   ],
   "id": "1bd34e958ab3e1b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "train single models",
   "id": "9631f051b7bff640"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "solver_params = dict(warn=False, score_func=score_func, n_targets_batch=1000)\n",
    "single_model = RidgeCV(alphas=alphas, cv=cv, solver_params=solver_params)\n",
    "single_model.fit(Xs[0][:n_samples_train], Y[:n_samples_train])\n",
    "score_0 = single_model.score(Xs[0][n_samples_train:], Y[n_samples_train:])\n",
    "score_0 = backend.to_numpy(score_0)"
   ],
   "id": "a094ee6c2190f080",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sns.displot(score_0)",
   "id": "4f5f89d21f4aebc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "solver_params = dict(warn=False, score_func=score_func, n_targets_batch=1000)\n",
    "single_model = RidgeCV(alphas=alphas, cv=cv, solver_params=solver_params)\n",
    "single_model.fit(Xs[1][:n_samples_train], Y[:n_samples_train])\n",
    "score_1 = single_model.score(Xs[1][n_samples_train:], Y[n_samples_train:])\n",
    "score_1 = backend.to_numpy(score_1)"
   ],
   "id": "100d05eb8b9ef93a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.histplot(score_1)\n",
    "plt.axvline(score_1.mean(), label=\"mean\", color=\"C1\")\n",
    "plt.legend()"
   ],
   "id": "cbe4b8c6c358476f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculate unique and shared variance",
   "id": "afc8eecd8c25de3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "shared = (score_0 + score_1) - joint_score\n",
    "x0_unique = score_0 - shared\n",
    "x1_unique = score_1 - shared"
   ],
   "id": "9e0802e4202b2455",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "shared = backend.to_numpy(shared)\n",
    "x0_unique = backend.to_numpy(x0_unique)\n",
    "x1_unique = backend.to_numpy(x1_unique)"
   ],
   "id": "19903ab20b12deac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "create pandas dataset of all scores for analysis",
   "id": "afbd9f3080698b94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "path = os.path.join('results', f'variance_partitioning_{scalars}.csv')\n",
    "if not os.path.exists(path):\n",
    "    scores = pd.DataFrame()\n",
    "    scores[r\"$(X_1\\cup X_2)\\beta \\approx Y$\"] = joint_score\n",
    "    scores[r\"$X_1\\beta \\approx Y$\"] = score_0\n",
    "    scores[r\"$X_2\\beta \\approx Y$\"] = score_1\n",
    "    scores[r\"$R^2(X_1\\beta \\approx Y)\\cap R^2(X_2\\beta \\approx Y)$\"] = shared\n",
    "    scores[r\"$R^2(X_1\\beta \\approx Y) \\setminus R^2(X_2\\beta \\approx Y)$\"] = x0_unique\n",
    "    scores[r\"$R^2(X_2\\beta \\approx Y) \\setminus R^2(X_1\\beta \\approx Y)$\"] = x1_unique\n",
    "    scores.to_csv(path)\n",
    "else:\n",
    "    scores = pd.read_csv(path)\n",
    "\n",
    "scores.head()"
   ],
   "id": "51160b7f8da4483f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "renamed_scores = pd.DataFrame()\n",
    "renamed_scores[\"A,B,C\"] = scores[r\"$(X_1\\cup X_2)\\beta \\approx Y$\"]\n",
    "renamed_scores[r\"A,B\"] = scores[r\"$X_1\\beta \\approx Y$\"]\n",
    "renamed_scores[r'A,C'] = scores[r\"$X_2\\beta \\approx Y$\"]\n",
    "renamed_scores[r\"A\"] = scores[r\"$R^2(X_1\\beta \\approx Y)\\cap R^2(X_2\\beta \\approx Y)$\"]\n",
    "renamed_scores[r\"B\"] = scores[r\"$R^2(X_1\\beta \\approx Y) \\setminus R^2(X_2\\beta \\approx Y)$\"]\n",
    "renamed_scores[r\"C\"] = scores[r\"$R^2(X_2\\beta \\approx Y) \\setminus R^2(X_1\\beta \\approx Y)$\"]\n",
    "\n",
    "renamed_scores.head()"
   ],
   "id": "4abd84cc8da3c08b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "sns.barplot(\n",
    "    data=renamed_scores,\n",
    "    palette=[\"C0\", \"C0\", \"C0\", \"C4\", \"C1\", \"C3\"],  # Define colors: one for joint/X₁/X₂, one for unique/shared\n",
    "    ax=ax,\n",
    "    errorbar='sd',\n",
    ")\n",
    "sns.despine(fig)\n",
    "theoretical_scores = [\n",
    "    sum(scalars),\n",
    "    (scalars[0] + scalars[1]),\n",
    "    (scalars[0] + scalars[2]),\n",
    "    scalars[0],\n",
    "    scalars[1],\n",
    "    scalars[2]\n",
    "]\n",
    "theoretical_scores = np.array(theoretical_scores) * (1 - noise_scalar)\n",
    "\n",
    "# Add lines indicating the maximum possible height for each bar\n",
    "for idx, column in enumerate(renamed_scores.columns):  # iterate over rows in the DataFrame\n",
    "    xmin = idx / len(renamed_scores.columns)  # Calculate xmin for each bar\n",
    "    xmax = (idx + 1) / len(renamed_scores.columns)  # Calculate xmax for each bar\n",
    "    plt.axhline(theoretical_scores[idx], linestyle='--', alpha=0.7, color='C0',\n",
    "                xmin=xmin, xmax=xmax, label=fr'EV' if idx == 0 else \"\")\n",
    "    plt.text(idx, renamed_scores[column].mean() / 2, f\"{renamed_scores[column].mean():.2f}\", ha='center', va='bottom',\n",
    "             fontsize=12)\n",
    "\n",
    "plt.xticks(range(len(renamed_scores.columns)), renamed_scores.columns, )\n",
    "\n",
    "plt.ylim(0, 1.01)\n",
    "\n",
    "# Ensure the legend is displayed properly\n",
    "plt.legend()\n",
    "plt.ylabel(r\"$R^2$ (avg. and sd)\")\n",
    "plt.xlabel(r\"Feature Space(s)\")"
   ],
   "id": "be80c029a5915b2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "simple_scores = pd.DataFrame()\n",
    "simple_scores[r\"$X_1\\cup X_2$\"] = scores[r\"$(X_1\\cup X_2)\\beta \\approx Y$\"]\n",
    "simple_scores[r\"$X_1$\"] = scores[r\"$X_1\\beta \\approx Y$\"]\n",
    "simple_scores[r\"$X_2$\"] = scores[r\"$X_2\\beta \\approx Y$\"]"
   ],
   "id": "bf44b5432ed7bfd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "sns.barplot(\n",
    "    data=simple_scores,\n",
    "    palette=[\"C0\", \"C2\", \"C2\"],  # Define colors: one for joint/X₁/X₂, one for unique/shared\n",
    "    ax=ax,\n",
    "    errorbar='sd',\n",
    ")\n",
    "sns.despine(fig)\n",
    "theoretical_scores = [\n",
    "    sum(scalars),\n",
    "    scalars[0] + scalars[1],\n",
    "    scalars[0] + scalars[2],\n",
    "]\n",
    "\n",
    "# Add lines indicating the maximum possible height for each bar\n",
    "for idx, column in enumerate(simple_scores.columns):  # iterate over rows in the DataFrame\n",
    "    xmin = idx / len(simple_scores.columns)  # Calculate xmin for each bar\n",
    "    xmax = (idx + 1) / len(simple_scores.columns)  # Calculate xmax for each bar\n",
    "    plt.axhline(theoretical_scores[idx], linestyle='--', alpha=0.7, color='C0',\n",
    "                xmin=xmin, xmax=xmax, label=fr'Expected Value' if idx == 0 else \"\")\n",
    "    plt.text(idx, simple_scores[column].mean() / 2, f\"{simple_scores[column].mean():.2f}\", ha='center', va='bottom')\n",
    "\n",
    "plt.xticks(range(len(simple_scores.columns)), simple_scores.columns)\n",
    "\n",
    "plt.ylim(0, 1.01)\n",
    "\n",
    "# Ensure the legend is displayed properly\n",
    "plt.legend()\n",
    "plt.ylabel(r\"$R^2$ (avg.)\")\n",
    "plt.xlabel(r\"Feature Space\")\n",
    "# plt.title(\"Variance Partitioning\")"
   ],
   "id": "e5457efd048e30ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vp_scores = pd.DataFrame()\n",
    "\n",
    "vp_scores[r'$\\mathbf{A}$'] = scores[r\"$R^2(X_1\\beta \\approx Y)\\cap R^2(X_2\\beta \\approx Y)$\"]\n",
    "vp_scores[r\"$\\mathbf{B}$\"] = scores[r\"$R^2(X_1\\beta \\approx Y) \\setminus R^2(X_2\\beta \\approx Y)$\"]\n",
    "vp_scores[r\"$\\mathbf{C}$\"] = scores[r\"$R^2(X_2\\beta \\approx Y) \\setminus R^2(X_1\\beta \\approx Y)$\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "sns.barplot(\n",
    "    data=vp_scores,\n",
    "    palette=[\"C4\", \"C1\", \"C3\"],  # Define colors: one for joint/X₁/X₂, one for unique/shared\n",
    "    ax=ax,\n",
    "    errorbar='sd',\n",
    ")\n",
    "sns.despine(fig)\n",
    "theoretical_scores = np.array([\n",
    "    scalars[0],\n",
    "    scalars[1],\n",
    "    scalars[2],\n",
    "]) * (1 - noise_scalar)\n",
    "\n",
    "# Add lines indicating the maximum possible height for each bar\n",
    "for idx, column in enumerate(vp_scores.columns):  # iterate over rows in the DataFrame\n",
    "    xmin = idx / len(vp_scores.columns)  # Calculate xmin for each bar\n",
    "    xmax = (idx + 1) / len(vp_scores.columns)  # Calculate xmax for each bar\n",
    "    plt.axhline(theoretical_scores[idx], linestyle='--', alpha=0.7, color='C0',\n",
    "                xmin=xmin, xmax=xmax, label=fr'Expected Value' if idx == 0 else \"\")\n",
    "    plt.text(idx, vp_scores[column].mean() / 2, f\"{vp_scores[column].mean():.2f}\", ha='center', va='center')\n",
    "\n",
    "# plt.xticks(range(len(vp_scores.columns)), vp_scores.columns)\n",
    "\n",
    "\n",
    "plt.ylim(0, 1.01)\n",
    "\n",
    "# Ensure the legend is displayed properly\n",
    "plt.legend()\n",
    "plt.ylabel(r\"$R^2$ (mean and sd)\")\n",
    "plt.xlabel(r\"Feature Space\")\n",
    "# plt.title(\"Variance Partitioning\")"
   ],
   "id": "eb06d696dc0d7d76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "sns.barplot(\n",
    "    data=scores,\n",
    "    palette=[\"C0\", \"C0\", \"C0\", \"C0\", \"C1\", \"C1\"],  # Define colors: one for joint/X₁/X₂, one for unique/shared\n",
    "    ax=ax,\n",
    "    errorbar='sd',\n",
    ")\n",
    "sns.despine(fig)\n",
    "theoretical_scores = [\n",
    "    sum(scalars),\n",
    "    scalars[0] + scalars[1],\n",
    "    scalars[0] + scalars[2],\n",
    "    scalars[0], scalars[1],\n",
    "    scalars[2]\n",
    "]\n",
    "\n",
    "# Add lines indicating the maximum possible height for each bar\n",
    "for idx, column in enumerate(scores.columns):  # iterate over rows in the DataFrame\n",
    "    xmin = idx / len(scores.columns)  # Calculate xmin for each bar\n",
    "    xmax = (idx + 1) / len(scores.columns)  # Calculate xmax for each bar\n",
    "    plt.axhline(theoretical_scores[idx], linestyle='--', alpha=0.7, color='C0',\n",
    "                xmin=xmin, xmax=xmax, label=fr'Expected Value' if idx == 0 else \"\")\n",
    "    plt.text(idx, scores[column].mean() / 2, f\"{scores[column].mean():.2f}\", ha='center', va='bottom')\n",
    "\n",
    "plt.xticks(range(len(scores.columns)), scores.columns, rotation=45, ha='right')\n",
    "\n",
    "plt.ylim(0, 1.01)\n",
    "\n",
    "# Ensure the legend is displayed properly\n",
    "plt.legend()\n",
    "plt.ylabel(r\"Variance Explained (avg. $R^2$ across targets)\")\n",
    "plt.xlabel(r\"Regression Model\")\n",
    "plt.title(\"Variance Partitioning\")"
   ],
   "id": "33ff0025fdc3ec36",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
