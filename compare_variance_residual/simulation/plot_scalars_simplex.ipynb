{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import simplstyles\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from himalaya.backend import set_backend\n",
    "from scipy.interpolate import griddata, CloughTocher2DInterpolator\n",
    "\n",
    "from compare_variance_residual.residual import residual_method\n",
    "from compare_variance_residual.simulation import generate_dataset\n",
    "from compare_variance_residual.variance_partitioning import variance_partitioning\n",
    "\n",
    "matplotlib.interactive(True)"
   ],
   "id": "590eef9c1328f71b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plt.style.use('nord-light-talk')",
   "id": "5c67f2cb1ce9ee73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def save_scores(scalars_list, d_list, n_targets, n_samples, noise_target, cv, alphas):\n",
    "    path = get_path(alphas, cv, n_targets)\n",
    "    for scalars in scalars_list:\n",
    "        print(scalars)\n",
    "        csv_path = os.path.join(path, f\"scores_{scalars}.csv\")\n",
    "        scores = pd.DataFrame()\n",
    "        if os.path.exists(csv_path):\n",
    "            print(\"skipping, already exists\")\n",
    "            continue\n",
    "        Xs, Y = generate_dataset(d_list, scalars, n_targets, n_samples, noise_target)\n",
    "        print(\"data generated\")\n",
    "        x1_score, x2_score, joint_score, x1_and_x2_score, vp_x1_unique_score, vp_x2_unique_score = variance_partitioning(\n",
    "            Xs, Y, n_samples_train, alphas, cv)\n",
    "        print(\"variance partitioning done\")\n",
    "\n",
    "        scores[\"x1_score\"] = x1_score\n",
    "        scores[\"x2_score\"] = x2_score\n",
    "        scores[\"vp_joint_score\"] = joint_score\n",
    "        scores[\"vp_shared_score\"] = x1_and_x2_score\n",
    "        scores[\"vp_x1_unique_score\"] = vp_x1_unique_score\n",
    "        scores[\"vp_x2_unique_score\"] = vp_x2_unique_score\n",
    "        del x1_score, x2_score, joint_score, x1_and_x2_score, vp_x1_unique_score, vp_x2_unique_score\n",
    "        print(scores.head())\n",
    "\n",
    "        x2_to_x1_score, x1_to_x2_score, rm_x1_unique_score, rm_x2_unique_score = residual_method(\n",
    "            Xs, Y, n_samples_train, alphas, cv)\n",
    "        print(\"residual method done\")\n",
    "        scores[\"rm_x2_to_x1_score\"] = np.concatenate(\n",
    "            [x2_to_x1_score, np.full(len(rm_x1_unique_score) - len(x2_to_x1_score), np.nan)])\n",
    "        scores[\"rm_x1_to_x2_score\"] = np.concatenate(\n",
    "            [x1_to_x2_score, np.full(len(rm_x1_unique_score) - len(x1_to_x2_score), np.nan)])\n",
    "        scores[\"rm_x1_unique_score\"] = rm_x1_unique_score\n",
    "        scores[\"rm_x2_unique_score\"] = rm_x2_unique_score\n",
    "        del x2_to_x1_score, x1_to_x2_score, rm_x1_unique_score, rm_x2_unique_score\n",
    "        del Xs, Y\n",
    "        scores.to_csv(csv_path, index=False)"
   ],
   "id": "6714beb7bfed56d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_path(alphas, cv, n_targets):\n",
    "    path = os.path.join(\"results\", f\"targets={n_targets}\", f\"cv={cv}\",\n",
    "                        f\"alphas={alphas.min()},{alphas.max()},{len(alphas)}\", \"varying scalars\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path"
   ],
   "id": "d4877733eb90d433"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_simplex(metric_values, interp_type='linear', fig=None, ax=None, min_total=None, max_total=None,\n",
    "                 cmap=plt.get_cmap().reversed(), plot_coords=False):\n",
    "    \"\"\"Plots a simplex heatmap based on provided metric values.\n",
    "\n",
    "    Args:\n",
    "        metric_values: A list or numpy array of metric values corresponding to each point in the simplex.\n",
    "                      The length must match the number of generated scalar points.\n",
    "        interp_type: Type of interpolation to use. Can be 'linear', 'clough_tocher', or 'cubic'. Defaults to 'linear'.\n",
    "    \"\"\"\n",
    "    if len(metric_values) != len(scalars_list):\n",
    "        raise ValueError(\"Length of metric_values must match the number of scalar points.\")\n",
    "\n",
    "    if interp_type not in ['linear', 'clough_tocher', 'cubic']:\n",
    "        raise ValueError(\"Invalid interpolation type. Choose 'linear', 'clough_tocher', or 'cubic'.\")\n",
    "\n",
    "    # Convert to barycentric coordinates\n",
    "    barycentric_coords = scalars_list\n",
    "\n",
    "    # Check if ax is provided or create a simplex plot\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    # Define simplex corners\n",
    "    corners = np.array([[0, 0], [1, 0], [0.5, np.sqrt(3) / 2]])\n",
    "\n",
    "    # Corner labels\n",
    "    corner_labels = ['(1, 0, 0)', '(0, 1, 0)', '(0, 0, 1)']\n",
    "\n",
    "    # Transform barycentric coordinates to Cartesian\n",
    "    cartesian_coords = np.dot(barycentric_coords, corners)\n",
    "\n",
    "    # Create grid for interpolation\n",
    "    resolution = 1000  # Adjust resolution as needed\n",
    "    x = np.linspace(0, 1, resolution)\n",
    "    y = np.linspace(0, np.sqrt(3) / 2, resolution)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    # Filter grid points that are outside the triangle\n",
    "    triangle = matplotlib.patches.Polygon(corners, closed=True)\n",
    "    mask = triangle.contains_points(grid_points)\n",
    "    grid_points_filtered = grid_points[mask]\n",
    "\n",
    "    # Interpolate values on the grid\n",
    "    if interp_type == 'linear':\n",
    "        interp_values = griddata(cartesian_coords, metric_values, grid_points_filtered, method='linear')\n",
    "    elif interp_type == 'clough_tocher':\n",
    "        interp = CloughTocher2DInterpolator(cartesian_coords, metric_values)\n",
    "        interp_values = interp(grid_points_filtered)\n",
    "    elif interp_type == 'cubic':\n",
    "        interp_values = griddata(cartesian_coords, metric_values, grid_points_filtered, method='cubic')\n",
    "\n",
    "    # Reshape interpolated values\n",
    "    zz = np.full_like(xx.ravel(), np.nan)\n",
    "    zz[mask] = interp_values\n",
    "    zz = zz.reshape(xx.shape)\n",
    "\n",
    "    # if total value ranges are given use those, otherwise not\n",
    "    if min_total is not None and max_total is not None:\n",
    "        vmin = min_total\n",
    "        vmax = max_total\n",
    "    else:\n",
    "        vmin = np.nanmin(zz)\n",
    "        vmax = np.nanmax(zz)\n",
    "\n",
    "    # Plot the colored triangle\n",
    "    im = ax.imshow(zz, extent=[0, 1, 0, np.sqrt(3) / 2], origin='lower', cmap=cmap, aspect='auto', vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # Add a colorbar\n",
    "    if fig is not None:\n",
    "        cbar = fig.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Metric Value')\n",
    "\n",
    "    # Draw the triangle boundaries (optional, if you want to highlight them)\n",
    "    ax.plot([corners[0, 0], corners[1, 0]], [corners[0, 1], corners[1, 1]], 'k-')\n",
    "    ax.plot([corners[1, 0], corners[2, 0]], [corners[1, 1], corners[2, 1]], 'k-')\n",
    "    ax.plot([corners[2, 0], corners[0, 0]], [corners[2, 1], corners[0, 1]], 'k-')\n",
    "\n",
    "    # plot points where values are plotted\n",
    "\n",
    "    if plot_coords:\n",
    "        for i, (x, y) in enumerate(cartesian_coords):\n",
    "            scalar_label = f\"({scalars_list[i][0]:.2f}, {scalars_list[i][1]:.2f}, {scalars_list[i][2]:.2f})\"\n",
    "            ax.text(x, y + 0.03, scalar_label, ha='center', va='center', fontsize=5, color='black')\n",
    "            ax.scatter(x, y, c=metric_values[i], cmap=cmap, vmin=vmin, vmax=vmax, marker='^')\n",
    "\n",
    "    # Add corner labels\n",
    "    halignments = ['left', 'right', 'center']\n",
    "    valignments = ['top', 'top', 'bottom']\n",
    "    for i, label in enumerate(corner_labels):\n",
    "        ax.text(corners[i, 0], corners[i, 1], corner_labels[i], ha=halignments[i], va=valignments[i])\n",
    "\n",
    "    # Remove x and y axes\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Remove spines\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    # remove background\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    return fig, ax, im"
   ],
   "id": "7d0155ea9daecf41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_barycentric_coordinates(x):\n",
    "    \"\"\"\n",
    "    Generates barycentric coordinates for a simplex with x subdivisions.\n",
    "\n",
    "    Args:\n",
    "        x: The number of subdivisions.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array of barycentric coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    coordinates = []\n",
    "    for i in range(x + 1):\n",
    "        for j in range(x - i + 1):\n",
    "            k = x - i - j\n",
    "            coordinates.append([i / x, j / x, k / x])\n",
    "\n",
    "    return np.array(coordinates)"
   ],
   "id": "3e31f69655d059a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate scalar points",
   "id": "e601ce39b14f1f92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "SUBDIVISIONS = 8",
   "id": "cd52c563a742e8fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scalars_list = generate_barycentric_coordinates(SUBDIVISIONS)\n",
    "print(len(scalars_list))\n",
    "scalars_list"
   ],
   "id": "3abb46648287748a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate data",
   "id": "4c2c6388cacc1787"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "backend = set_backend(\"cupy\", on_error=\"warn\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(42)"
   ],
   "id": "add80fd0bdc5ccc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "d_list = [100, 100, 100]\n",
    "n_targets = 10000\n",
    "n_samples_train = 10000\n",
    "n_samples_test = 100\n",
    "n_samples = n_samples_train + n_samples_test\n",
    "noise_target = 0.1\n",
    "\n",
    "cv = 10\n",
    "alphas = np.logspace(-5, 5, 10)\n",
    "path = get_path(alphas, cv, n_targets)"
   ],
   "id": "75d5869b8f3b5304"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "save_scores(scalars_list, d_list, n_targets, n_samples, noise_target, cv, alphas)",
   "id": "78e4a2d2c64c2604"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "plot points on triangle simplex",
   "id": "e8ce7e15309fa261"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_simplex(np.random.rand(len(scalars_list)), plot_coords=True)",
   "id": "2a7a78ddf5c0350a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# plot mean error",
   "id": "b0972a346691d937"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vp_x1_mean_error_predicted = []\n",
    "vp_x2_mean_error_predicted = []\n",
    "rm_x1_mean_error_predicted = []\n",
    "rm_x2_mean_error_predicted = []\n",
    "\n",
    "vp_x1_std_errors = []\n",
    "vp_x2_std_errors = []\n",
    "rm_x1_std_errors = []\n",
    "rm_x2_std_errors = []\n",
    "\n",
    "for scalars in scalars_list:\n",
    "    scores = pd.read_csv(\n",
    "        os.path.join(get_path(alphas, cv, n_targets), f\"scores_{scalars}.csv\"))\n",
    "    print(\"read scores for \", scalars)\n",
    "\n",
    "    # calculate error\n",
    "    vp_x1_error = scores[\"vp_x1_unique_score\"] - scalars[1]\n",
    "    vp_x2_error = scores[\"vp_x2_unique_score\"] - scalars[2]\n",
    "    rm_x1_error = scores[\"rm_x1_unique_score\"] - scalars[1]\n",
    "    rm_x2_error = scores[\"rm_x2_unique_score\"] - scalars[2]\n",
    "\n",
    "    vp_x1_mean_error_predicted.append(vp_x1_error.mean())\n",
    "    vp_x2_mean_error_predicted.append(vp_x2_error.mean())\n",
    "    rm_x1_mean_error_predicted.append(rm_x1_error.mean())\n",
    "    rm_x2_mean_error_predicted.append(rm_x2_error.mean())\n",
    "    vp_x1_std_errors.append(vp_x1_error.std())\n",
    "    vp_x2_std_errors.append(vp_x2_error.std())\n",
    "    rm_x1_std_errors.append(rm_x1_error.std())\n",
    "    rm_x2_std_errors.append(rm_x2_error.std())"
   ],
   "id": "b8ce84f2d6d8bd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "min_total = min(min(vp_x1_mean_error_predicted), min(rm_x1_mean_error_predicted))\n",
    "max_total = max(max(vp_x1_mean_error_predicted), max(rm_x1_mean_error_predicted))\n",
    "\n",
    "_, _, im_vp_x1 = plot_simplex(vp_x1_mean_error_predicted, ax=axs[0], min_total=min_total, max_total=max_total)\n",
    "_, _, im_rm_x1 = plot_simplex(rm_x1_mean_error_predicted, ax=axs[1], min_total=min_total, max_total=max_total)\n",
    "\n",
    "# add colorbar based on max values to figure\n",
    "cbar = fig.colorbar(im_vp_x1, ax=axs)\n",
    "cbar.set_label('Prediction Error')\n",
    "\n",
    "# add column wise titles\n",
    "axs[0].set_title(r'Variance Partitioning', y=1.05, fontsize=12)\n",
    "axs[1].set_title(r'Residual Method', y=1.05, fontsize=12)"
   ],
   "id": "15605fd1cce4eaf4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(13, 10))\n",
    "\n",
    "min_total = min(min(vp_x1_mean_error_predicted), min(vp_x2_mean_error_predicted), min(rm_x1_mean_error_predicted),\n",
    "                min(rm_x2_mean_error_predicted))\n",
    "max_total = max(max(vp_x1_mean_error_predicted), max(vp_x2_mean_error_predicted), max(rm_x1_mean_error_predicted),\n",
    "                max(rm_x2_mean_error_predicted))\n",
    "\n",
    "_, _, im_vp_x1 = plot_simplex(vp_x1_mean_error_predicted, ax=axs[0, 0], min_total=min_total, max_total=max_total)\n",
    "_, _, im_vp_x2 = plot_simplex(vp_x2_mean_error_predicted, ax=axs[0, 1], min_total=min_total, max_total=max_total)\n",
    "_, _, im_rm_x1 = plot_simplex(rm_x1_mean_error_predicted, ax=axs[1, 0], min_total=min_total, max_total=max_total)\n",
    "_, _, im_rm_x2 = plot_simplex(rm_x2_mean_error_predicted, ax=axs[1, 1], min_total=min_total, max_total=max_total)\n",
    "\n",
    "# add colorbar based on max values to figure\n",
    "cbar = fig.colorbar(im_vp_x1, ax=axs)\n",
    "cbar.set_label('Mean Error Across Targets')\n",
    "\n",
    "# add column wise titles\n",
    "axs[0, 0].set_title(r'Predicted Unique Contribution of $X_1$', y=1.05, fontsize=12)\n",
    "axs[0, 1].set_title(r'Predicted Unique Contribution of $X_2$', y=1.05, fontsize=12)\n",
    "\n",
    "# add row titles\n",
    "axs[0, 0].set_ylabel(r'Variance Partitioning', fontsize=12)\n",
    "axs[1, 0].set_ylabel(r'Residual Method', fontsize=12)"
   ],
   "id": "ec3f44bbac4699ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(13, 10))\n",
    "\n",
    "min_total = min(min(vp_x1_std_errors), min(vp_x2_std_errors), min(rm_x1_std_errors), min(rm_x2_std_errors))\n",
    "max_total = max(max(vp_x1_std_errors), max(vp_x2_std_errors), max(rm_x1_std_errors), max(rm_x2_std_errors))\n",
    "\n",
    "cmap = plt.get_cmap('magma')\n",
    "_, _, im_vp_x1 = plot_simplex(vp_x1_std_errors, ax=axs[0, 0], min_total=min_total, max_total=max_total, cmap=cmap)\n",
    "_, _, im_vp_x2 = plot_simplex(vp_x2_std_errors, ax=axs[0, 1], min_total=min_total, max_total=max_total, cmap=cmap)\n",
    "_, _, im_rm_x1 = plot_simplex(rm_x1_std_errors, ax=axs[1, 0], min_total=min_total, max_total=max_total, cmap=cmap)\n",
    "_, _, im_rm_x2 = plot_simplex(rm_x2_std_errors, ax=axs[1, 1], min_total=min_total, max_total=max_total, cmap=cmap)\n",
    "\n",
    "# add colorbar based on max values to figure\n",
    "cbar = fig.colorbar(im_vp_x1, ax=axs, cmap='magma')\n",
    "cbar.set_label('Standard Error Across Targets')\n",
    "\n",
    "# add column wise titles\n",
    "axs[0, 0].set_title(r'Predicted Unique Contribution of $X_1$', y=1.05, fontsize=12)\n",
    "axs[0, 1].set_title(r'Predicted Unique Contribution of $X_2$', y=1.05, fontsize=12)\n",
    "\n",
    "# add row titles\n",
    "axs[0, 0].set_ylabel(r'Variance Partitioning', fontsize=12)\n",
    "axs[1, 0].set_ylabel(r'Residual Method', fontsize=12)"
   ],
   "id": "e3f3d5150aa9f861"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plot only a a line on the simplex",
   "id": "8dee3ae8729f1109"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "shared_scalar = 0.5",
   "id": "6b9c05f784b9c291"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# find all paths with this shared scalar\n",
    "experiment_dir = get_path(alphas, cv, n_targets)\n",
    "all_paths = os.listdir(experiment_dir)\n",
    "filtered_paths = []\n",
    "for path in all_paths:\n",
    "    if path.__contains__(f\"[{shared_scalar}\"):\n",
    "        filtered_paths.append(path)\n",
    "filtered_paths"
   ],
   "id": "d18984dfcf346f14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# extract scalars from paths\n",
    "interp_points = np.array(\n",
    "    [np.fromstring(path.replace(\"scores_[\", \"\").replace(\"].csv\", \"\"), dtype=float, count=3, sep=' ') for path in\n",
    "     filtered_paths]\n",
    ")\n",
    "interp_points = interp_points[interp_points[:, 1].argsort()]\n",
    "interp_points"
   ],
   "id": "32b48ebdccf1f23a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "save_scores(interp_points, d_list, n_targets, n_samples, noise_target, cv, alphas)",
   "id": "61f6a406b23b57ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# plot error for variance partitioning and residual method\n",
    "vp_x1_mean_error_predicted = []\n",
    "vp_x1_mse = []\n",
    "rm_x1_mean_error_predicted = []\n",
    "rm_x1_mse = []\n",
    "\n",
    "for scalars in interp_points:\n",
    "    scores = pd.read_csv(os.path.join(get_path(alphas, cv, n_targets), f\"scores_{scalars}.csv\"))\n",
    "    vp_predicted = scores[\"vp_x1_unique_score\"]\n",
    "    rm_predicted = scores[\"rm_x1_unique_score\"]\n",
    "    vp_x1_mean_error_predicted.append(vp_predicted.mean())\n",
    "    rm_x1_mean_error_predicted.append(rm_predicted.mean())\n",
    "    vp_x1_mse.append((vp_predicted - scalars[1]).std())\n",
    "    rm_x1_mse.append((rm_predicted - scalars[1]).std())\n",
    "\n",
    "vp_x1_mean_error_predicted, vp_x1_mse, rm_x1_mean_error_predicted, rm_x1_mse = map(np.array, (\n",
    "    vp_x1_mean_error_predicted, vp_x1_mse, rm_x1_mean_error_predicted, rm_x1_mse))\n",
    "\n",
    "vp = pd.DataFrame(\n",
    "    {\n",
    "        'true_scores': interp_points[:, 1],\n",
    "        'avg': vp_x1_mean_error_predicted,\n",
    "        'mse': vp_x1_mse,\n",
    "        'lower_mse': vp_x1_mean_error_predicted - vp_x1_mse,\n",
    "        'upper_mse': vp_x1_mean_error_predicted + vp_x1_mse\n",
    "    }\n",
    ")\n",
    "rm = pd.DataFrame(\n",
    "    {\n",
    "        'true_scores': interp_points[:, 1],\n",
    "        'avg': rm_x1_mean_error_predicted,\n",
    "        'mse': rm_x1_mse,\n",
    "        'lower_mse': rm_x1_mean_error_predicted - rm_x1_mse,\n",
    "        'upper_mse': rm_x1_mean_error_predicted + rm_x1_mse,\n",
    "    }\n",
    ")\n",
    "vp"
   ],
   "id": "c57bc2de81764072"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# create line plot following b_scalar\n",
    "fig, axs = plt.subplots(figsize=(3 * len(interp_points), 5))\n",
    "x = range(len(interp_points))\n",
    "\n",
    "# plot line for each point in edges\n",
    "plt.plot(x, interp_points[:, 1], label=f\"True Contribution\", color='black')\n",
    "\n",
    "# add xticks of points\n",
    "plt.xticks(x, [f\"({x[0]:.3f}, {x[1]:.3f}, {x[2]:.3f})\" for x in interp_points])\n",
    "\n",
    "# plot error\n",
    "plt.plot(x, vp['avg'], label=\"Variance Partitioning\")\n",
    "plt.plot(x, rm['avg'], label=\"Residual Method\")\n",
    "plt.fill_between(x, vp['lower_mse'], vp['upper_mse'], alpha=0.3)\n",
    "plt.fill_between(x, rm['lower_mse'], rm['upper_mse'], alpha=0.3)\n",
    "# plt.errorbar(x, vp_x1_mean_error_predicted, yerr=vp_x1_mse)\n",
    "# plt.errorbar(x, rm_x1_mean_error_predicted, yerr=rm_x1_mse)\n",
    "\n",
    "# Annotate each point for Variance Partitioning\n",
    "for i in x:\n",
    "    plt.text(i, vp['avg'][i], f\"{vp['true_scores'][i]:.2f}\", fontsize=10, ha='center',\n",
    "             va='bottom', color='C0')\n",
    "    plt.text(i, rm['avg'][i], f\"{rm['true_scores'][i]:.2f}\", fontsize=10, ha='center',\n",
    "             va='top', color='C1')\n",
    "\n",
    "# add labels\n",
    "plt.xlabel(r\"True Contributions ($a_\\mathbf{A}, a_\\mathbf{B}, a_\\mathbf{C}$)\")\n",
    "plt.ylabel(\"Predicted Contribution (avg.)\")\n",
    "plt.title(r\"Predicted Unique Contribution of $X_1$ (Large Shared Scalar)\")\n",
    "\n",
    "plt.legend()"
   ],
   "id": "e2bb16c87ce71a9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b6d15f9b4c7571b8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
