{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Variance Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T15:00:11.610199Z",
     "start_time": "2024-12-17T15:00:10.961634Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from himalaya.backend import set_backend, get_backend\n",
    "from himalaya.kernel_ridge import KernelRidgeCV, MultipleKernelRidgeCV\n",
    "from himalaya.kernel_ridge import Kernelizer\n",
    "from himalaya.kernel_ridge import ColumnKernelizer\n",
    "from himalaya.kernel_ridge import generate_dirichlet_samples\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this example, we use the ``cupy`` backend (GPU).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T15:00:11.814713Z",
     "start_time": "2024-12-17T15:00:11.613765Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "backend = set_backend(\"cupy\", on_error=\"warn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T15:00:11.903761Z",
     "start_time": "2024-12-17T15:00:11.898163Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_multikernel_dataset(n_kernels=2, n_targets=500,\n",
    "                                 n_samples_train=1000, n_samples_test=400,\n",
    "                                 noise=0.1, kernel_weights=None, overlap=0.8,\n",
    "                                 n_features_list=None, random_state=None):\n",
    "    \"\"\"Utility to generate datasets for the gallery of examples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_kernels : int\n",
    "        Number of kernels.\n",
    "    n_targets : int\n",
    "        Number of targets.\n",
    "    n_samples_train : int\n",
    "        Number of samples in the training set.\n",
    "    n_samples_test : int\n",
    "        Number of sample in the testing set.\n",
    "    noise : float > 0\n",
    "        Scale of the Gaussian white noise added to the targets.\n",
    "    kernel_weights : array of shape (n_targets, n_kernels) or None\n",
    "        Kernel weights used in the prediction of the targets.\n",
    "        If None, generate random kernel weights from a Dirichlet distribution.\n",
    "    overlap: float >= 0\n",
    "    \n",
    "    n_features_list : list of int of length (n_kernels, ) or None\n",
    "        Number of features in each kernel. If None, use 1000 features for each.\n",
    "    random_state : int, or None\n",
    "        Random generator seed use to generate the true kernel weights.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Xs_train : array of shape (n_feature_spaces, n_samples_train, n_features)\n",
    "        Training features.\n",
    "    Xs_test : array of shape (n_feature_spaces, n_samples_test, n_features)\n",
    "        Testing features.\n",
    "    Y_train : array of shape (n_samples_train, n_targets)\n",
    "        Training targets.\n",
    "    Y_test : array of shape (n_samples_test, n_targets)\n",
    "        Testing targets.\n",
    "    kernel_weights : array of shape (n_targets, n_kernels)\n",
    "        Kernel weights in the prediction of the targets.\n",
    "    n_features_list : list of int of length (n_kernels, )\n",
    "        Number of features in each kernel.\n",
    "    \"\"\"\n",
    "    from himalaya.utils import check_random_state\n",
    "\n",
    "    backend = get_backend()\n",
    "\n",
    "    # Create a few kernel weights if not given.\n",
    "    if kernel_weights is None:\n",
    "        kernel_weights = generate_dirichlet_samples(n_targets, n_kernels,\n",
    "                                                    concentration=[.3],\n",
    "                                                    random_state=random_state)\n",
    "        kernel_weights = backend.to_numpy(kernel_weights)\n",
    "\n",
    "    if n_features_list is None:\n",
    "        n_features_list = np.full(n_kernels, fill_value=1000)\n",
    "\n",
    "    rng = check_random_state(random_state)\n",
    "\n",
    "    # Then, generate a random dataset, using the arbitrary scalings.\n",
    "    Xs_train, Xs_test = [], []\n",
    "    Y_train, Y_test = None, None\n",
    "\n",
    "    # Generate a shared component Z\n",
    "    Z_train = rng.rand(n_samples_train, 1)\n",
    "    Z_test = rng.rand(n_samples_test, 1)\n",
    "\n",
    "    for ii in range(n_kernels):\n",
    "        n_features = n_features_list[ii]\n",
    "\n",
    "        X_train = rng.randn(n_samples_train, n_features)\n",
    "        X_test = rng.randn(n_samples_test, n_features)\n",
    "        X_train = overlap * Z_train + (1 - overlap) * X_train\n",
    "        X_test = overlap * Z_test + (1 - overlap) * X_test\n",
    "        X_train -= X_train.mean(0)\n",
    "        X_test -= X_test.mean(0)\n",
    "        Xs_train.append(X_train)\n",
    "        Xs_test.append(X_test)\n",
    "\n",
    "        weights = rng.randn(n_features, n_targets) / n_features\n",
    "        weights *= kernel_weights[:, ii] ** 0.5\n",
    "\n",
    "        if ii == 0:\n",
    "            Y_train = X_train @ weights\n",
    "            Y_test = X_test @ weights\n",
    "        else:\n",
    "            Y_train += X_train @ weights\n",
    "            Y_test += X_test @ weights\n",
    "\n",
    "    std = Y_train.std(0)[None]\n",
    "    Y_train /= std\n",
    "    Y_test /= std\n",
    "\n",
    "    Y_train += rng.randn(n_samples_train, n_targets) * noise\n",
    "    Y_test += rng.randn(n_samples_test, n_targets) * noise\n",
    "    Y_train -= Y_train.mean(0)\n",
    "    Y_test -= Y_test.mean(0)\n",
    "\n",
    "    # Concatenate the feature spaces.\n",
    "    Xs_train = backend.asarray(Xs_train, dtype=\"float32\")\n",
    "    Xs_test = backend.asarray(Xs_test, dtype=\"float32\")\n",
    "    Y_train = backend.asarray(Y_train, dtype=\"float32\")\n",
    "    Y_test = backend.asarray(Y_test, dtype=\"float32\")\n",
    "    kernel_weights = backend.asarray(kernel_weights, dtype=\"float32\")\n",
    "\n",
    "    return Xs_train, Xs_test, Y_train, Y_test, kernel_weights, n_features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Generate a random dataset\n",
    "- X_train : array of shape (n_samples_train, n_features)\n",
    "- X_test : array of shape (n_samples_test, n_features)\n",
    "- Y_train : array of shape (n_samples_train, n_targets)\n",
    "- Y_test : array of shape (n_samples_test, n_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T15:00:11.943828Z",
     "start_time": "2024-12-17T15:00:11.941152Z"
    }
   },
   "outputs": [],
   "source": [
    "n_kernels = 2\n",
    "n_features_list = [1000, 1000]\n",
    "n_targets = 1000\n",
    "n_samples_train = 600\n",
    "n_samples_test = 300\n",
    "kernel_weights = [[0.5, 0.5]] * n_targets\n",
    "kernel_weights = np.array(kernel_weights)\n",
    "overlap = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-17T15:00:11.991334Z"
    },
    "collapsed": false,
    "jupyter": {
     "is_executing": true,
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "(Xs_train, Xs_test, Y_train, Y_test, kernel_weights, n_features_list) = generate_multikernel_dataset(\n",
    "    n_kernels=n_kernels, n_features_list=n_features_list, n_targets=n_targets,\n",
    "    n_samples_train=n_samples_train,\n",
    "    n_samples_test=n_samples_test,\n",
    "    kernel_weights=kernel_weights,\n",
    "    overlap=overlap,\n",
    "    random_state=42)\n",
    "\n",
    "feature_names = [f\"Feature space {ii}\" for ii in range(len(n_features_list))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Variance Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Single models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.560109977Z",
     "start_time": "2024-12-17T13:23:19.226733Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "solver_params = dict()\n",
    "\n",
    "single_pipelines = []\n",
    "for feature in range(n_kernels):\n",
    "    single_model = KernelRidgeCV(alphas=np.logspace(-10, 10, 41), kernel=\"precomputed\", solver=\"eigenvalues\",\n",
    "                                 solver_params=solver_params)\n",
    "\n",
    "    pipe = make_pipeline(Kernelizer(), single_model)\n",
    "    single_pipelines.append(pipe)\n",
    "    pipe.fit(Xs_train[feature], Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Compute prediction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.561132041Z",
     "start_time": "2024-12-17T13:23:19.604496Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# predict for all targets\n",
    "test_scores = []\n",
    "for i, pipeline in enumerate(single_pipelines):\n",
    "    score = pipeline.score(Xs_test[i], Y_test)\n",
    "    score = backend.to_numpy(score)\n",
    "    test_scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Plot voxelwise R^2 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.561642417Z",
     "start_time": "2024-12-17T13:23:19.727069Z"
    }
   },
   "outputs": [],
   "source": [
    "maximum = 0\n",
    "for score in test_scores:\n",
    "    maximum = max(maximum, score.max())\n",
    "\n",
    "bins = np.linspace(0, maximum, 50)\n",
    "for i, score in enumerate(test_scores):\n",
    "    plt.hist(score, bins, alpha=0.7, label=f\"{feature_names[i]}\")\n",
    "\n",
    "plt.xlabel(r\"$R^2$ generalization score\")\n",
    "plt.title(\"Histogram over targets\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Joint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.578226761Z",
     "start_time": "2024-12-17T13:23:19.970730Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack([X for X in Xs_train])\n",
    "X_test = np.hstack([X for X in Xs_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.582712589Z",
     "start_time": "2024-12-17T13:23:20.071049Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find the start and end of each feature space X in Xs\n",
    "start_and_end = np.concatenate([[0], np.cumsum(n_features_list)])\n",
    "slices = [\n",
    "    slice(start, end)\n",
    "    for start, end in zip(start_and_end[:-1], start_and_end[1:])\n",
    "]\n",
    "\n",
    "# Create a different ``Kernelizer`` for each feature space.\n",
    "kernelizers = [(\"space %d\" % ii, Kernelizer(), slice_)\n",
    "               for ii, slice_ in enumerate(slices)]\n",
    "column_kernelizer = ColumnKernelizer(kernelizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Define the random-search model\n",
    "We use very few iteration on purpose, to make the random search suboptimal,\n",
    "and refine it with hyperparameter gradient descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.583618433Z",
     "start_time": "2024-12-17T13:23:20.173389Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "solver_params = dict(n_iter=5, alphas=np.logspace(-10, 10, 41))\n",
    "\n",
    "model_random = MultipleKernelRidgeCV(kernels=\"precomputed\", solver=\"random_search\",\n",
    "                                     solver_params=solver_params, random_state=42)\n",
    "pipe_random = make_pipeline(column_kernelizer, model_random)\n",
    "\n",
    "# Fit the model on all targets\n",
    "pipe_random.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Define the gradient-descent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.596081208Z",
     "start_time": "2024-12-17T13:23:20.710456Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "solver_params = dict(max_iter=10, hyper_gradient_method=\"direct\",\n",
    "                     max_iter_inner_hyper=10,\n",
    "                     initial_deltas=\"here_will_go_the_previous_deltas\")\n",
    "\n",
    "model_gradient = MultipleKernelRidgeCV(kernels=\"precomputed\", solver=\"hyper_gradient\",\n",
    "                                       solver_params=solver_params)\n",
    "pipe_gradient = make_pipeline(column_kernelizer, model_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Use the random-search to initialize the gradient-descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.596946595Z",
     "start_time": "2024-12-17T13:23:20.918062Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We might want to refine only the best predicting targets, since the\n",
    "# hyperparameter gradient descent is less efficient over many targets.\n",
    "top = 100  # top 60%\n",
    "best_cv_scores = backend.to_numpy(pipe_random[-1].cv_scores_.max(0))\n",
    "mask = best_cv_scores > np.percentile(best_cv_scores, 100 - top)\n",
    "\n",
    "deltas = pipe_random[-1].deltas_\n",
    "# deltas = deltas[:, mask]\n",
    "\n",
    "pipe_gradient[-1].solver_params['initial_deltas'] = deltas\n",
    "# pipe_2.fit(X_train, Y_train[:, mask])\n",
    "pipe_gradient.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Compute predictions on a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.597811793Z",
     "start_time": "2024-12-17T13:23:25.192071Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use the first model for all targets\n",
    "test_scores_random = pipe_random.score(X_test, Y_test)\n",
    "\n",
    "# use the second model for the refined targets\n",
    "# test_scores_2 = backend.copy(test_scores_1)\n",
    "# test_scores_2[mask] = pipe_2.score(X_test, Y_test[:, mask])\n",
    "test_scores_refined = pipe_gradient.score(X_test, Y_test)\n",
    "\n",
    "test_scores_random = backend.to_numpy(test_scores_random)\n",
    "test_scores_refined = backend.to_numpy(test_scores_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.598017452Z",
     "start_time": "2024-12-17T13:23:25.278562Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(test_scores_random, test_scores_refined, alpha=0.3)\n",
    "plt.xlim(0, 1)\n",
    "plt.plot(plt.xlim(), plt.xlim(), color='k', lw=1)\n",
    "plt.xlabel(r\"Base model\")\n",
    "plt.ylabel(r\"Refined model\")\n",
    "plt.title(\"$R^2$ generalization score\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.598217641Z",
     "start_time": "2024-12-17T13:23:25.464493Z"
    }
   },
   "outputs": [],
   "source": [
    "maximum = test_scores_refined.max()\n",
    "for score in test_scores:\n",
    "    maximum = max(maximum, score.max())\n",
    "\n",
    "bins = np.linspace(0, maximum, 50)\n",
    "for score, feature in zip(test_scores, feature_names):\n",
    "    plt.hist(score, bins, alpha=0.7, label=feature)\n",
    "plt.hist(test_scores_refined, bins, alpha=.7, label=\"Joint Model\")\n",
    "\n",
    "plt.xlabel(r\"$R^2$ generalization score\")\n",
    "plt.title(\"Histogram over targets\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.598435864Z",
     "start_time": "2024-12-17T13:23:25.729561Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(1, len(test_scores), figsize=(4 * len(test_scores), 4))\n",
    "\n",
    "# Loop through each score and corresponding subplot\n",
    "for i, score in enumerate(test_scores):\n",
    "    ax = axs[i]\n",
    "    ax.scatter(score, test_scores_refined, alpha=0.3)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.plot(ax.get_xlim(), ax.get_xlim(), color='k', lw=1)\n",
    "    ax.set_xlabel(f\"{feature_names[i]}\")\n",
    "    ax.set_ylabel(r\"Joint model\")\n",
    "    ax.set_title(\"$R^2$ generalization score\")\n",
    "    ax.grid()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.598611877Z",
     "start_time": "2024-12-17T13:23:25.958040Z"
    }
   },
   "outputs": [],
   "source": [
    "# single model predictions\n",
    "corrs_single = []\n",
    "for i, pipeline in enumerate(single_pipelines):\n",
    "    prediction = pipeline.predict(Xs_test[i])\n",
    "\n",
    "    correlation = backend.asarray([np.corrcoef(prediction[:, ii].ravel(), Y_test[:, ii])[0, 1]\n",
    "                                   for ii in range(Y_test.shape[1])])\n",
    "    correlation = backend.to_numpy(correlation)\n",
    "    corrs_single.append(correlation)\n",
    "\n",
    "prediction = pipe_gradient.predict(X_test)\n",
    "corr_joint = backend.asarray([np.corrcoef(prediction[:, ii].ravel(), Y_test[:, ii])[0, 1]\n",
    "                              for ii in range(Y_test.shape[1])])\n",
    "corr_joint = backend.to_numpy(corr_joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.598787840Z",
     "start_time": "2024-12-17T13:23:27.372716Z"
    }
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 1, 50)\n",
    "\n",
    "for corr, feature in zip(corrs_single, feature_names):\n",
    "    plt.hist(corr, bins, alpha=.7, label=feature)\n",
    "\n",
    "plt.hist(corr_joint, bins, alpha=.7, label=\"Joint Model\")\n",
    "\n",
    "plt.xlabel(r\"$\\rho$ prediction correlation\")\n",
    "plt.title(\"Histogram over targets\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute unique and shared variance using R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.598962170Z",
     "start_time": "2024-12-17T13:23:27.635570Z"
    }
   },
   "outputs": [],
   "source": [
    "zero_minus_one = test_scores_refined - test_scores[1]\n",
    "one_minus_zero = test_scores_refined - test_scores[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute unique and shared variance using correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.599121461Z",
     "start_time": "2024-12-17T13:23:27.704740Z"
    }
   },
   "outputs": [],
   "source": [
    "def ssc(data: np.array):\n",
    "    \"\"\"\n",
    "    Calculate the signed squared correlation of a matrix\n",
    "    :param data: np.array\n",
    "    :return: np.array\n",
    "    \"\"\"\n",
    "    # return data ** 2\n",
    "    return (data ** 2) * np.sign(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.599284279Z",
     "start_time": "2024-12-17T13:23:27.838622Z"
    }
   },
   "outputs": [],
   "source": [
    "# estimate the explained variance of each model using signed squared correlation\n",
    "squared_intersection = ssc(corrs_single[0]) + ssc(corrs_single[1]) - ssc(corr_joint)\n",
    "squared_variance_a_minus_b = ssc(corrs_single[0]) - squared_intersection\n",
    "squared_variance_b_minus_a = ssc(corrs_single[1]) - squared_intersection\n",
    "\n",
    "# take roots of the squared values\n",
    "intersection = np.sqrt(squared_intersection)\n",
    "variance_a_minus_b = np.sqrt(squared_variance_a_minus_b)\n",
    "variance_b_minus_a = np.sqrt(squared_variance_b_minus_a)\n",
    "\n",
    "# remove nans\n",
    "variance_a_minus_b = np.nan_to_num(variance_a_minus_b)\n",
    "variance_b_minus_a = np.nan_to_num(variance_b_minus_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.599483897Z",
     "start_time": "2024-12-17T13:23:27.844662Z"
    }
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 1, 50)\n",
    "\n",
    "plt.hist(variance_a_minus_b, bins, alpha=.7, label=\"Feature space 0 \\\\ Feature space 1\")\n",
    "plt.hist(variance_b_minus_a, bins, alpha=.7, label=\"Feature space 1 \\\\ Feature space 0\")\n",
    "plt.hist(intersection, bins, alpha=.7, label=\"Intersection\")\n",
    "\n",
    "plt.xlabel(r\"$\\rho$ prediction correlation\")\n",
    "plt.title(\"Histogram over targets\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare predictions before and after removing shared variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T14:59:37.599671432Z",
     "start_time": "2024-12-17T13:23:28.109292Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(1, len(test_scores), figsize=(4 * len(test_scores), 4))\n",
    "\n",
    "# Loop through each score and corresponding subplot\n",
    "for i, (corr, corr_unique) in enumerate(zip(corrs_single, [variance_a_minus_b, variance_b_minus_a])):\n",
    "    ax = axs[i]\n",
    "    ax.scatter(corr, corr_unique, alpha=0.3)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.plot(ax.get_xlim(), ax.get_xlim(), color='k', lw=1)\n",
    "    ax.set_xlabel(f\"{feature_names[i]}\")\n",
    "    ax.set_ylabel(f\"{feature_names[i]} \\\\ ({feature_names[0]}$\\cap${feature_names[1]})\")\n",
    "    ax.set_title(r\"Combinded vs. unique $\\rho$\")\n",
    "    ax.grid()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
