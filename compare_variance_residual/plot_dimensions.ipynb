{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:27:19.815602Z",
     "start_time": "2025-02-19T19:27:19.722548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from himalaya.backend import set_backend\n",
    "\n",
    "from compare_variance_residual.residual import residual_method\n",
    "from compare_variance_residual.simulation import generate_dataset\n",
    "from compare_variance_residual.variance_partitioning import variance_partitioning\n",
    "\n",
    "\n",
    "def get_path(alphas, cv, n_samples, n_targets, noise_target):\n",
    "    path = os.path.join(\"results\", f\"targets={n_targets}\", f\"samples={n_samples}\", f\"noise={noise_target}\", f\"cv={cv}\",\n",
    "                        f\"alphas={alphas.min()},{alphas.max()},{len(alphas)}\", \"varying dimensions\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path"
   ],
   "id": "7214bbfbd22e42b9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:27:19.898023Z",
     "start_time": "2025-02-19T19:27:19.893499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_scores(d_list_list, scalars, n_targets, n_samples, noise_target, cv, alphas):\n",
    "    path = get_path(alphas, cv, n_samples, n_targets, noise_target)\n",
    "    for d_list in d_list_list:\n",
    "        print(d_list)\n",
    "        csv_path = os.path.join(path, f\"scores_{d_list}.csv\")\n",
    "        scores = pd.DataFrame()\n",
    "        if os.path.exists(csv_path):\n",
    "            print(\"skipping, already exists\")\n",
    "            continue\n",
    "        Xs, Y = generate_dataset(d_list, scalars, n_targets, n_samples, noise_target)\n",
    "        print(\"data generated\")\n",
    "        x1_score, x2_score, joint_score, x1_and_x2_score, vp_x1_unique_score, vp_x2_unique_score = variance_partitioning(\n",
    "            Xs, Y, n_samples_train, alphas, cv)\n",
    "        print(\"variance partitioning done\")\n",
    "\n",
    "        scores[\"x1_score\"] = x1_score\n",
    "        scores[\"x2_score\"] = x2_score\n",
    "        scores[\"vp_joint_score\"] = joint_score\n",
    "        scores[\"vp_shared_score\"] = x1_and_x2_score\n",
    "        scores[\"vp_x1_unique_score\"] = vp_x1_unique_score\n",
    "        scores[\"vp_x2_unique_score\"] = vp_x2_unique_score\n",
    "        del x1_score, x2_score, joint_score, x1_and_x2_score, vp_x1_unique_score, vp_x2_unique_score\n",
    "        print(scores.head())\n",
    "\n",
    "        _, _, x2_to_x1_score, x1_to_x2_score, rm_x1_unique_score, rm_x2_unique_score = residual_method(\n",
    "            Xs, Y, n_samples_train, alphas, cv)\n",
    "        print(\"residual method done\")\n",
    "        scores[\"rm_x2_to_x1_score\"] = np.concatenate(\n",
    "            [x2_to_x1_score, np.full(len(rm_x1_unique_score) - len(x2_to_x1_score), np.nan)])\n",
    "        scores[\"rm_x1_to_x2_score\"] = np.concatenate(\n",
    "            [x1_to_x2_score, np.full(len(rm_x1_unique_score) - len(x1_to_x2_score), np.nan)])\n",
    "        scores[\"rm_x1_unique_score\"] = rm_x1_unique_score\n",
    "        scores[\"rm_x2_unique_score\"] = rm_x2_unique_score\n",
    "        del x2_to_x1_score, x1_to_x2_score, rm_x1_unique_score, rm_x2_unique_score\n",
    "        del Xs, Y\n",
    "        scores.to_csv(csv_path, index=False)"
   ],
   "id": "e892fc1cda6d373f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save scores for varying Dimensions",
   "id": "29652e5a7524e7c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:27:20.133431Z",
     "start_time": "2025-02-19T19:27:19.945970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "backend = set_backend(\"cupy\", on_error=\"warn\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(42)"
   ],
   "id": "d88dce6a0ca359b5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:27:20.141639Z",
     "start_time": "2025-02-19T19:27:20.139190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_list = [100, 100, 100]\n",
    "n_targets = 10000\n",
    "n_samples_train = 10000\n",
    "n_samples_test = 100\n",
    "n_samples = n_samples_train + n_samples_test\n",
    "noise_target = 0.1\n",
    "scalars_list = [1 / 3, 1 / 3, 1 / 3]\n",
    "\n",
    "cv = 10\n",
    "alphas = np.logspace(-5, 5, 10)"
   ],
   "id": "7d22e5e47c2bd337",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:27:20.188500Z",
     "start_time": "2025-02-19T19:27:20.186127Z"
    }
   },
   "cell_type": "code",
   "source": "varying_dim = np.logspace(1, 3, 10, dtype=int)\n",
   "id": "d13ef57f64a8417f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Shared dimension",
   "id": "b806e6a9da8060c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:27:20.234535Z",
     "start_time": "2025-02-19T19:27:20.232290Z"
    }
   },
   "cell_type": "code",
   "source": "d_list_list = [[dim, d_list[1], d_list[2]] for dim in varying_dim]",
   "id": "f06754f09aab7974",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-19T19:27:20.284951Z"
    }
   },
   "cell_type": "code",
   "source": "save_scores(d_list_list, d_list, n_targets, n_samples, noise_target, cv, alphas)",
   "id": "5115fc2bfda7cdc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.int64(10), 100, 100]\n",
      "data generated\n",
      "variance partitioning done\n",
      "   x1_score  x2_score  vp_joint_score  vp_shared_score  vp_x1_unique_score  \\\n",
      "0  0.541758  0.687327        0.919659         0.309426            0.232332   \n",
      "1  0.470424  0.549045        0.863074         0.156395            0.314029   \n",
      "2  0.583584  0.681983        0.895606         0.369961            0.213623   \n",
      "3  0.596878  0.602543        0.884479         0.314942            0.281936   \n",
      "4  0.538621  0.614795        0.879366         0.274050            0.264572   \n",
      "\n",
      "   vp_x2_unique_score  \n",
      "0            0.377901  \n",
      "1            0.392650  \n",
      "2            0.312022  \n",
      "3            0.287601  \n",
      "4            0.340745  \n",
      "residual method done\n",
      "[np.int64(16), 100, 100]\n",
      "data generated\n",
      "variance partitioning done\n",
      "   x1_score  x2_score  vp_joint_score  vp_shared_score  vp_x1_unique_score  \\\n",
      "0  0.633584  0.495717        0.900534         0.228766            0.404818   \n",
      "1  0.635178  0.667356        0.898666         0.403869            0.231310   \n",
      "2  0.583003  0.595394        0.904313         0.274084            0.308919   \n",
      "3  0.657777  0.539163        0.864586         0.332354            0.325423   \n",
      "4  0.555758  0.564965        0.910469         0.210253            0.345505   \n",
      "\n",
      "   vp_x2_unique_score  \n",
      "0            0.266950  \n",
      "1            0.263487  \n",
      "2            0.321310  \n",
      "3            0.206809  \n",
      "4            0.354711  \n",
      "residual method done\n",
      "[np.int64(27), 100, 100]\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vp_x1_unique_predicted = []\n",
    "vp_x2_unique_predicted = []\n",
    "rm_x1_unique_predicted = []\n",
    "rm_x2_unique_predicted = []\n",
    "\n",
    "for d_list in d_list_list:\n",
    "    scores = pd.read_csv(os.path.join(get_path(alphas, cv, n_samples, n_targets, noise_target), f\"scores_{d_list}.csv\"))\n",
    "    vp_x1_unique_predicted.append(scores[\"vp_x1_unique_score\"])\n",
    "    vp_x2_unique_predicted.append(scores[\"vp_x2_unique_score\"])\n",
    "    rm_x1_unique_predicted.append(scores[\"rm_x1_unique_score\"])\n",
    "    rm_x2_unique_predicted.append(scores[\"rm_x2_unique_score\"])"
   ],
   "id": "5d1b00874386e234"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sns.catplot(data=pd.DataFrame(vp_x1_unique_predicted).T)",
   "id": "584dbe676ed596d1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
