{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T12:28:08.353193Z",
     "start_time": "2025-02-22T12:28:05.345442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import time as tm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import json"
   ],
   "id": "2fe61507fb3cd56a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "CACHE_DIR = \"../cache\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ],
   "id": "be37331a9ea98231",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_model(model_name):\n",
    "    with open('../text_model_config.json', 'r') as f:\n",
    "        model_config = json.load(f)[model_name]\n",
    "    model_hf_path = model_config['huggingface_hub']\n",
    "    model = AutoModel.from_pretrained(model_hf_path, cache_dir=CACHE_DIR).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_hf_path, cache_dir=CACHE_DIR)\n",
    "    model.eval()\n",
    "    return model, model_config, tokenizer\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_model_layer_representations(model_name, text_array):\n",
    "    model, model_config, tokenizer = load_model(model_name)\n",
    "\n",
    "    # get the token embeddings\n",
    "    token_embeddings = []\n",
    "    for word in text_array:\n",
    "        current_token_embedding = get_model_token_embeddings([word], tokenizer, model)\n",
    "        token_embeddings.append(np.mean(current_token_embedding.detach().numpy(), 1))\n",
    "\n",
    "    # layer-wise embeddings of particular length\n",
    "    words_layers_representations = {}\n",
    "    n_total_layers = model_config['num_layers']\n",
    "    for layer in range(n_total_layers):\n",
    "        words_layers_representations[layer] = []\n",
    "    words_layers_representations[-1] = token_embeddings\n",
    "\n",
    "    words_layers_representations = extract_context_representations(model, model_config, text_array, tokenizer,\n",
    "                                                                   words_layers_representations)\n",
    "    return words_layers_representations\n",
    "\n",
    "\n",
    "def extract_context_representations(model, model_config, text_array, tokenizer, words_layers_representations):\n",
    "    # Before we've seen enough words to make up the seq_len\n",
    "    # Extract index 0 after supplying tokens 0 to 0, extract 1 after 0 to 1, 2 after 0 to 2, ... , 19 after 0 to 19\n",
    "    start_time = tm.time()\n",
    "    for seq_len in range(len(text_array)):\n",
    "        if seq_len < sequence_length:\n",
    "            word_seq = text_array[:seq_len + 1]\n",
    "            extract_index = seq_len\n",
    "        else:\n",
    "            word_seq = text_array[seq_len - sequence_length + 1:seq_len + 1]\n",
    "            extract_index = sequence_length - 1\n",
    "\n",
    "        words_layers_representations = add_avrg_token_embedding_for_specific_word(word_seq, tokenizer, model,\n",
    "                                                                                  extract_index,\n",
    "                                                                                  words_layers_representations,\n",
    "                                                                                  model_config)\n",
    "\n",
    "        if seq_len % 100 == 0:\n",
    "            print('Completed {} out of {}: {}'.format(seq_len, len(text_array), tm.time() - start_time))\n",
    "            start_time = tm.time()\n",
    "    print(f'Done extracting sequences of length {sequence_length}')\n",
    "    return words_layers_representations\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_model_embeddings(words_in_array, tokenizer, model, model_config):\n",
    "    \"\"\"\n",
    "    extracts layer representations for all words in words_in_array\n",
    "    :param encoded_layers: list of tensors, length num layers. each tensor of dims num tokens by num dimensions in representation\n",
    "    :param word_ind_to_token_ind: dict that maps from index in words_in_array to index in array of tokens when words_in_array is tokenized,\n",
    "                      with keys: index of word, and values: array of indices of corresponding tokens when word is tokenized\n",
    "    \"\"\"\n",
    "    n_seq_tokens = 0\n",
    "    seq_tokens = []\n",
    "\n",
    "    word_ind_to_token_ind = {}  # dict that maps index of word in words_in_array to index of tokens in seq_tokens\n",
    "\n",
    "    for i, word in enumerate(words_in_array):\n",
    "        word_ind_to_token_ind[i] = []  # initialize token indices array for current word\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "        for token in word_tokens:\n",
    "            #             if token not in remove_chars:  # don't add any tokens that are in remove_chars\n",
    "            seq_tokens.append(token)\n",
    "            word_ind_to_token_ind[i].append(n_seq_tokens)\n",
    "            n_seq_tokens = n_seq_tokens + 1\n",
    "\n",
    "    # convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(seq_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
    "\n",
    "    if model_config['model_type'] == 'encoder' or model_config['model_type'] == 'decoder':\n",
    "        outputs = model(tokens_tensor, output_hidden_states=True)\n",
    "        hidden_states = outputs['hidden_states'][1:]  # This is a tuple: (layer1, layer2, ..., layer6)\n",
    "        all_layers_hidden_states = hidden_states\n",
    "    elif model_config['model_type'] == 'encoder-decoder':\n",
    "        outputs = model(tokens_tensor, decoder_input_ids=tokens_tensor, output_hidden_states=True)\n",
    "        encoder_hidden_states = outputs['encoder_hidden_states'][1:]  # This is a tuple: (layer1, layer2, ..., layer6)\n",
    "        decoder_hidden_states = outputs['decoder_hidden_states'][1:]\n",
    "        all_layers_hidden_states = encoder_hidden_states + decoder_hidden_states\n",
    "    else:\n",
    "        raise ValueError(\"model_type should be either encoder, decoder or encoder-decoder\")\n",
    "\n",
    "    return all_layers_hidden_states, word_ind_to_token_ind, None\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def add_word_model_embedding(model_dict, embeddings_to_add, token_inds_to_avrg, specific_layer=-1):\n",
    "    \"\"\"\n",
    "    add the embeddings for a specific word in the sequence\n",
    "\n",
    "    :param token_inds_to_avrg: indices of tokens in embeddings output to avrg\n",
    "    \"\"\"\n",
    "    if specific_layer >= 0:  # only add embeddings for one specified layer\n",
    "        layer_embedding = embeddings_to_add[specific_layer]\n",
    "        full_sequence_embedding = layer_embedding.cpu().detach().numpy()\n",
    "        model_dict[specific_layer].append(np.mean(full_sequence_embedding[0, token_inds_to_avrg, :], 0))\n",
    "    else:\n",
    "        for layer, layer_embedding in enumerate(embeddings_to_add):\n",
    "            full_sequence_embedding = layer_embedding.cpu().detach().numpy()\n",
    "            model_dict[layer].append(np.mean(full_sequence_embedding[0, token_inds_to_avrg, :],\n",
    "                                             0))  # avrg over all tokens for specified word\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def add_avrg_token_embedding_for_specific_word(word_seq, tokenizer, model, from_start_word_ind_to_extract, model_dict,\n",
    "                                               model_config):\n",
    "    \"\"\"\n",
    "    predicts representations for specific word in input word sequence, ad adds to existing layer-wise dictionary\n",
    "\n",
    "    :param word_seq: numpy array of words in input sequence\n",
    "    :param tokenizer: Auto tokenizer\n",
    "    :param model: Auto model\n",
    "    :param from_start_word_ind_to_extract: the index of the word whose features to extract, INDEXED FROM START OF WORD_SEQ\n",
    "    :param model_dict: where to save the extracted embeddings\n",
    "    \"\"\"\n",
    "    word_seq = list(word_seq)\n",
    "    all_sequence_embeddings, word_ind_to_token_ind, _ = predict_model_embeddings(word_seq, tokenizer, model,\n",
    "                                                                                 model_config)\n",
    "    token_inds_to_avrg = word_ind_to_token_ind[from_start_word_ind_to_extract]\n",
    "    model_dict = add_word_model_embedding(model_dict, all_sequence_embeddings, token_inds_to_avrg)\n",
    "\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_model_token_embeddings(words_in_array, tokenizer, model):\n",
    "    n_seq_tokens = 0\n",
    "    seq_tokens = []\n",
    "\n",
    "    word_ind_to_token_ind = {}  # dict that maps index of word in words_in_array to index of tokens in seq_tokens\n",
    "\n",
    "    for i, word in enumerate(words_in_array):\n",
    "        word_ind_to_token_ind[i] = []  # initialize token indices array for current word\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "        for token in word_tokens:\n",
    "            #             if token not in remove_chars:  # don't add any tokens that are in remove_chars\n",
    "            seq_tokens.append(token)\n",
    "            word_ind_to_token_ind[i].append(n_seq_tokens)\n",
    "            n_seq_tokens = n_seq_tokens + 1\n",
    "\n",
    "    # convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(seq_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
    "\n",
    "    input_embedding_module = model.base_model.get_input_embeddings()\n",
    "    token_embeddings = input_embedding_module(tokens_tensor.to(torch.long)).cpu()\n",
    "\n",
    "    return token_embeddings"
   ],
   "id": "47bd9c482e7a8777",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Extract Context Representations",
   "id": "1c1061fdaca81ea9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stories_path = \"../stimuli/formatted\"\n",
    "sequence_length = 20\n",
    "model_name = \"bert-base\"\n",
    "representations_file = f\"../{model_name}{sequence_length}.npy\""
   ],
   "id": "4fbf8776733eeb0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if os.path.exists(representations_file):\n",
    "    print(\"File already exists\")\n",
    "else:\n",
    "    stories_files = {}\n",
    "    for story in sorted(os.listdir(stories_path)):\n",
    "        print(story)\n",
    "        story_path = os.path.join(stories_path, story)\n",
    "        words = open(story_path, 'r').read().strip().split('\\n')\n",
    "        embeddings = get_model_layer_representations(model_name, np.array(words))\n",
    "        stories_files[story] = embeddings\n",
    "\n",
    "    np.save(representations_file, stories_files)"
   ],
   "id": "544ba05b7acc9633",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Downsample representations to match fMRI data",
   "id": "c5ee14d76a31bd7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "grid_dir = \"../stimuli/grids\"\n",
    "trfiles_dir = \"../stimuli/trfiles\"\n",
    "data_dir = \"../data\"\n",
    "downsampled_semanticseqs_file = f\"../{model_name}{sequence_length}_downsampled.npy\""
   ],
   "id": "ab4db4f94f1e3ad0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from compare_variance_residual.stimuli_utils.textgrid import TextGrid\n",
    "\n",
    "stimul_features = np.load(representations_file, allow_pickle=True)\n",
    "\n",
    "training_story_names = ['alternateithicatom', 'avatar', 'howtodraw', 'legacy',\n",
    "                        'life', 'myfirstdaywiththeyankees', 'naked',\n",
    "                        'odetostepfather', 'souls', 'undertheinfluence']\n",
    "# Pstories are the test (or Prediction) stories (well, story), which we will use to test our models\n",
    "prediction_story_names = ['wheretheressmoke']\n",
    "all_story_names = training_story_names + prediction_story_names\n",
    "all_story_names"
   ],
   "id": "631d0d0886bc601c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "grids = {}\n",
    "for story in all_story_names:\n",
    "    print(story)\n",
    "    gridfile = [os.path.join(grid_dir, gf) for gf in os.listdir(grid_dir) if gf.startswith(story)][0]\n",
    "    with open(gridfile) as f:\n",
    "        grids[story] = TextGrid(f.read())"
   ],
   "id": "b5443c9caa6e152c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from compare_variance_residual.stimuli_utils.trfile import TRFile\n",
    "\n",
    "trfiles = dict()\n",
    "\n",
    "for story in all_story_names:\n",
    "    try:\n",
    "        trf = TRFile(os.path.join(trfiles_dir, \"%s.report\" % story))\n",
    "        trfiles[story] = [trf]\n",
    "    except Exception as e:\n",
    "        print(e)"
   ],
   "id": "d65ad2c3204683e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from compare_variance_residual.stimuli_utils.SemtanticModel import SemanticModel\n",
    "from ridge_utils.dsutils import make_word_ds\n",
    "\n",
    "# Make word and phoneme datasequences\n",
    "wordseqs = make_word_ds(grids, trfiles)  # dictionary of {storyname : word DataSequence}\n",
    "eng1000 = SemanticModel.load(os.path.join(data_dir, \"english1000sm.hf5\"))"
   ],
   "id": "583a44d9b751eaff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ridge_utils.dsutils import make_semantic_model\n",
    "\n",
    "# semanticseqs = dict()  # dictionary to hold projected stimuli {story name : projected DataSequence}\n",
    "# for story in all_story_names:\n",
    "#     semanticseqs[story] = make_semantic_model(wordseqs[story], [eng1000], [985])\n",
    "story_filenames = ['alternateithicatom', 'avatar', 'howtodraw', 'legacy',\n",
    "                   'life', 'myfirstdaywiththeyankees', 'naked',\n",
    "                   'odetostepfather', 'souls', 'undertheinfluence', 'wheretheressmoke']\n",
    "semanticseqs = dict()\n",
    "for i in np.arange(len(all_story_names)):\n",
    "    temp = make_semantic_model(wordseqs[all_story_names[i]], [eng1000], [985])\n",
    "    temp.data = np.nan_to_num(stimul_features.item()[story_filenames[i]][layer])\n",
    "    semanticseqs[all_story_names[i]] = temp"
   ],
   "id": "a1eb1ba38e84ec1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Downsample stimuli\n",
    "interptype = \"lanczos\"  # filter type\n",
    "window = 3  # number of lobes in Lanczos filter\n",
    "downsampled_semanticseqs = dict()  # dictionary to hold downsampled stimuli\n",
    "for story in all_story_names:\n",
    "    downsampled_semanticseqs[story] = semanticseqs[story].chunksums(interptype, window=window)\n",
    "\n",
    "np.save(downsampled_semanticseqs_file, downsampled_semanticseqs)"
   ],
   "id": "47e5405a13f9c9b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
