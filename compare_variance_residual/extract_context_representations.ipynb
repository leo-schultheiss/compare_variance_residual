{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:32:09.893679Z",
     "start_time": "2025-02-21T18:32:09.891132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import time as tm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import json"
   ],
   "id": "2fe61507fb3cd56a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:32:09.908798Z",
     "start_time": "2025-02-21T18:32:09.906606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CACHE_DIR = \"../cache\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ],
   "id": "be37331a9ea98231",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:32:09.954716Z",
     "start_time": "2025-02-21T18:32:09.950124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_model(model_name):\n",
    "    with open('../text_model_config.json', 'r') as f:\n",
    "        model_config = json.load(f)[model_name]\n",
    "    model_hf_path = model_config['huggingface_hub']\n",
    "    model = AutoModel.from_pretrained(model_hf_path, cache_dir=CACHE_DIR).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_hf_path, cache_dir=CACHE_DIR)\n",
    "    model.eval()\n",
    "    return model, model_config, tokenizer\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_model_layer_representations(model_name, text_array):\n",
    "    model, model_config, tokenizer = load_model(model_name)\n",
    "\n",
    "    # get the token embeddings\n",
    "    token_embeddings = []\n",
    "    for word in text_array:\n",
    "        current_token_embedding = get_model_token_embeddings([word], tokenizer, model)\n",
    "        token_embeddings.append(np.mean(current_token_embedding.detach().numpy(), 1))\n",
    "\n",
    "    # layer-wise embeddings of particular length\n",
    "    words_layers_representations = {}\n",
    "    n_total_layers = model_config['num_layers']\n",
    "    for layer in range(n_total_layers):\n",
    "        words_layers_representations[layer] = []\n",
    "    words_layers_representations[-1] = token_embeddings\n",
    "\n",
    "    words_layers_representations = extract_context_representations(model, model_config, text_array, tokenizer,\n",
    "                                                                   words_layers_representations)\n",
    "    return words_layers_representations\n",
    "\n",
    "\n",
    "def extract_context_representations(model, model_config, text_array, tokenizer, words_layers_representations):\n",
    "    # Before we've seen enough words to make up the seq_len\n",
    "    # Extract index 0 after supplying tokens 0 to 0, extract 1 after 0 to 1, 2 after 0 to 2, ... , 19 after 0 to 19\n",
    "    start_time = tm.time()\n",
    "    for seq_len in range(len(text_array)):\n",
    "        if seq_len < sequence_length:\n",
    "            word_seq = text_array[:seq_len + 1]\n",
    "            extract_index = seq_len\n",
    "        else:\n",
    "            word_seq = text_array[seq_len - sequence_length + 1:seq_len + 1]\n",
    "            extract_index = sequence_length - 1\n",
    "\n",
    "        words_layers_representations = add_avrg_token_embedding_for_specific_word(word_seq, tokenizer, model,\n",
    "                                                                                  extract_index,\n",
    "                                                                                  words_layers_representations,\n",
    "                                                                                  model_config)\n",
    "\n",
    "        if seq_len % 100 == 0:\n",
    "            print('Completed {} out of {}: {}'.format(seq_len, len(text_array), tm.time() - start_time))\n",
    "            start_time = tm.time()\n",
    "    print(f'Done extracting sequences of length {sequence_length}')\n",
    "    return words_layers_representations\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_model_embeddings(words_in_array, tokenizer, model, model_config):\n",
    "    \"\"\"\n",
    "    extracts layer representations for all words in words_in_array\n",
    "    :param encoded_layers: list of tensors, length num layers. each tensor of dims num tokens by num dimensions in representation\n",
    "    :param word_ind_to_token_ind: dict that maps from index in words_in_array to index in array of tokens when words_in_array is tokenized,\n",
    "                      with keys: index of word, and values: array of indices of corresponding tokens when word is tokenized\n",
    "    \"\"\"\n",
    "    n_seq_tokens = 0\n",
    "    seq_tokens = []\n",
    "\n",
    "    word_ind_to_token_ind = {}  # dict that maps index of word in words_in_array to index of tokens in seq_tokens\n",
    "\n",
    "    for i, word in enumerate(words_in_array):\n",
    "        word_ind_to_token_ind[i] = []  # initialize token indices array for current word\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "        for token in word_tokens:\n",
    "            #             if token not in remove_chars:  # don't add any tokens that are in remove_chars\n",
    "            seq_tokens.append(token)\n",
    "            word_ind_to_token_ind[i].append(n_seq_tokens)\n",
    "            n_seq_tokens = n_seq_tokens + 1\n",
    "\n",
    "    # convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(seq_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
    "\n",
    "    if model_config['model_type'] == 'encoder' or model_config['model_type'] == 'decoder':\n",
    "        outputs = model(tokens_tensor, output_hidden_states=True)\n",
    "        hidden_states = outputs['hidden_states'][1:]  # This is a tuple: (layer1, layer2, ..., layer6)\n",
    "        all_layers_hidden_states = hidden_states\n",
    "    elif model_config['model_type'] == 'encoder-decoder':\n",
    "        outputs = model(tokens_tensor, decoder_input_ids=tokens_tensor, output_hidden_states=True)\n",
    "        encoder_hidden_states = outputs['encoder_hidden_states'][1:]  # This is a tuple: (layer1, layer2, ..., layer6)\n",
    "        decoder_hidden_states = outputs['decoder_hidden_states'][1:]\n",
    "        all_layers_hidden_states = encoder_hidden_states + decoder_hidden_states\n",
    "    else:\n",
    "        raise ValueError(\"model_type should be either encoder, decoder or encoder-decoder\")\n",
    "\n",
    "    return all_layers_hidden_states, word_ind_to_token_ind, None\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def add_word_model_embedding(model_dict, embeddings_to_add, token_inds_to_avrg, specific_layer=-1):\n",
    "    \"\"\"\n",
    "    add the embeddings for a specific word in the sequence\n",
    "\n",
    "    :param token_inds_to_avrg: indices of tokens in embeddings output to avrg\n",
    "    \"\"\"\n",
    "    if specific_layer >= 0:  # only add embeddings for one specified layer\n",
    "        layer_embedding = embeddings_to_add[specific_layer]\n",
    "        full_sequence_embedding = layer_embedding.cpu().detach().numpy()\n",
    "        model_dict[specific_layer].append(np.mean(full_sequence_embedding[0, token_inds_to_avrg, :], 0))\n",
    "    else:\n",
    "        for layer, layer_embedding in enumerate(embeddings_to_add):\n",
    "            full_sequence_embedding = layer_embedding.cpu().detach().numpy()\n",
    "            model_dict[layer].append(np.mean(full_sequence_embedding[0, token_inds_to_avrg, :],\n",
    "                                             0))  # avrg over all tokens for specified word\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def add_avrg_token_embedding_for_specific_word(word_seq, tokenizer, model, from_start_word_ind_to_extract, model_dict,\n",
    "                                               model_config):\n",
    "    \"\"\"\n",
    "    predicts representations for specific word in input word sequence, ad adds to existing layer-wise dictionary\n",
    "\n",
    "    :param word_seq: numpy array of words in input sequence\n",
    "    :param tokenizer: Auto tokenizer\n",
    "    :param model: Auto model\n",
    "    :param from_start_word_ind_to_extract: the index of the word whose features to extract, INDEXED FROM START OF WORD_SEQ\n",
    "    :param model_dict: where to save the extracted embeddings\n",
    "    \"\"\"\n",
    "    word_seq = list(word_seq)\n",
    "    all_sequence_embeddings, word_ind_to_token_ind, _ = predict_model_embeddings(word_seq, tokenizer, model,\n",
    "                                                                                 model_config)\n",
    "    token_inds_to_avrg = word_ind_to_token_ind[from_start_word_ind_to_extract]\n",
    "    model_dict = add_word_model_embedding(model_dict, all_sequence_embeddings, token_inds_to_avrg)\n",
    "\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_model_token_embeddings(words_in_array, tokenizer, model):\n",
    "    n_seq_tokens = 0\n",
    "    seq_tokens = []\n",
    "\n",
    "    word_ind_to_token_ind = {}  # dict that maps index of word in words_in_array to index of tokens in seq_tokens\n",
    "\n",
    "    for i, word in enumerate(words_in_array):\n",
    "        word_ind_to_token_ind[i] = []  # initialize token indices array for current word\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "        for token in word_tokens:\n",
    "            #             if token not in remove_chars:  # don't add any tokens that are in remove_chars\n",
    "            seq_tokens.append(token)\n",
    "            word_ind_to_token_ind[i].append(n_seq_tokens)\n",
    "            n_seq_tokens = n_seq_tokens + 1\n",
    "\n",
    "    # convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(seq_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
    "\n",
    "    input_embedding_module = model.base_model.get_input_embeddings()\n",
    "    token_embeddings = input_embedding_module(tokens_tensor.to(torch.long)).cpu()\n",
    "\n",
    "    return token_embeddings"
   ],
   "id": "47bd9c482e7a8777",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:32:10.062963Z",
     "start_time": "2025-02-21T18:32:10.060714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stories_path = \"../stimuli/formatted\"\n",
    "sequence_length = 20\n",
    "model_name = \"bert-base\""
   ],
   "id": "4fbf8776733eeb0c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-21T18:32:10.106381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stories_files = {}\n",
    "\n",
    "for story in sorted(os.listdir(stories_path)):\n",
    "    print(story)\n",
    "    story_path = os.path.join(stories_path, story)\n",
    "    words = open(story_path, 'r').read().strip().split('\\n')\n",
    "    embeddings = get_model_layer_representations(model_name, np.array(words))\n",
    "    stories_files[story] = embeddings\n",
    "\n",
    "representations_file = f\"../{model_name}{sequence_length}.npy\"\n",
    "\n",
    "np.save(representations_file, stories_files)"
   ],
   "id": "544ba05b7acc9633",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 out of 2174: 0.06339645385742188\n",
      "Completed 100 out of 2174: 1.1833176612854004\n",
      "Completed 200 out of 2174: 1.2587106227874756\n",
      "Completed 300 out of 2174: 1.2614953517913818\n",
      "Completed 400 out of 2174: 1.423375129699707\n",
      "Completed 500 out of 2174: 1.2984960079193115\n",
      "Completed 600 out of 2174: 1.248481273651123\n",
      "Completed 700 out of 2174: 1.260920524597168\n",
      "Completed 800 out of 2174: 1.6794636249542236\n",
      "Completed 900 out of 2174: 1.6352174282073975\n",
      "Completed 1000 out of 2174: 1.4942166805267334\n",
      "Completed 1100 out of 2174: 1.2715380191802979\n",
      "Completed 1200 out of 2174: 1.2464196681976318\n",
      "Completed 1300 out of 2174: 1.6140148639678955\n",
      "Completed 1400 out of 2174: 1.3652715682983398\n",
      "Completed 1500 out of 2174: 1.2580795288085938\n",
      "Completed 1600 out of 2174: 1.2339119911193848\n",
      "Completed 1700 out of 2174: 1.2492051124572754\n",
      "Completed 1800 out of 2174: 1.2491569519042969\n",
      "Completed 1900 out of 2174: 1.2544851303100586\n",
      "Completed 2000 out of 2174: 1.2539114952087402\n",
      "Completed 2100 out of 2174: 1.3074908256530762\n",
      "Done extracting sequences of length 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/PycharmProjects/compare_variance_residual/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/leo/PycharmProjects/compare_variance_residual/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:137: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 out of 1469: 0.0052814483642578125\n",
      "Completed 100 out of 1469: 1.1907095909118652\n",
      "Completed 200 out of 1469: 1.2589871883392334\n",
      "Completed 300 out of 1469: 1.2447457313537598\n",
      "Completed 400 out of 1469: 1.240922451019287\n",
      "Completed 500 out of 1469: 1.2494430541992188\n",
      "Completed 600 out of 1469: 1.3523526191711426\n",
      "Completed 700 out of 1469: 1.7443833351135254\n",
      "Completed 800 out of 1469: 1.4565179347991943\n",
      "Completed 900 out of 1469: 1.2875912189483643\n",
      "Completed 1000 out of 1469: 1.2749862670898438\n",
      "Completed 1100 out of 1469: 1.2701287269592285\n",
      "Completed 1200 out of 1469: 1.3223965167999268\n",
      "Completed 1300 out of 1469: 1.3785960674285889\n",
      "Completed 1400 out of 1469: 1.3351938724517822\n",
      "Done extracting sequences of length 20\n",
      "Completed 0 out of 1964: 0.006074666976928711\n",
      "Completed 100 out of 1964: 1.1743550300598145\n",
      "Completed 200 out of 1964: 1.3231728076934814\n",
      "Completed 300 out of 1964: 1.2568612098693848\n",
      "Completed 400 out of 1964: 1.2506580352783203\n",
      "Completed 500 out of 1964: 1.2434241771697998\n",
      "Completed 600 out of 1964: 1.2472801208496094\n",
      "Completed 700 out of 1964: 1.4021151065826416\n",
      "Completed 800 out of 1964: 1.4348046779632568\n",
      "Completed 900 out of 1964: 1.372359037399292\n",
      "Completed 1000 out of 1964: 1.2644829750061035\n",
      "Completed 1100 out of 1964: 1.2817003726959229\n",
      "Completed 1200 out of 1964: 1.2908766269683838\n",
      "Completed 1300 out of 1964: 1.2972664833068848\n",
      "Completed 1400 out of 1964: 1.2894134521484375\n",
      "Completed 1500 out of 1964: 1.2382123470306396\n",
      "Completed 1600 out of 1964: 1.257218360900879\n",
      "Completed 1700 out of 1964: 1.2416198253631592\n",
      "Completed 1800 out of 1964: 1.259401559829712\n",
      "Completed 1900 out of 1964: 1.2517976760864258\n",
      "Done extracting sequences of length 20\n",
      "Completed 0 out of 1893: 0.005172252655029297\n",
      "Completed 100 out of 1893: 1.1622846126556396\n",
      "Completed 200 out of 1893: 1.2690682411193848\n",
      "Completed 300 out of 1893: 1.300081729888916\n",
      "Completed 400 out of 1893: 1.3504259586334229\n",
      "Completed 500 out of 1893: 1.3216981887817383\n",
      "Completed 600 out of 1893: 1.5107612609863281\n",
      "Completed 700 out of 1893: 1.6072728633880615\n",
      "Completed 800 out of 1893: 1.5144779682159424\n",
      "Completed 900 out of 1893: 1.2471153736114502\n",
      "Completed 1000 out of 1893: 1.3310754299163818\n",
      "Completed 1100 out of 1893: 1.6935656070709229\n",
      "Completed 1200 out of 1893: 1.6085026264190674\n",
      "Completed 1300 out of 1893: 1.411989688873291\n",
      "Completed 1400 out of 1893: 1.2753136157989502\n",
      "Completed 1500 out of 1893: 1.2627780437469482\n",
      "Completed 1600 out of 1893: 1.256352424621582\n",
      "Completed 1700 out of 1893: 1.2542307376861572\n",
      "Completed 1800 out of 1893: 1.2563543319702148\n",
      "Done extracting sequences of length 20\n",
      "Completed 0 out of 2209: 0.00615692138671875\n",
      "Completed 100 out of 2209: 1.1880443096160889\n",
      "Completed 200 out of 2209: 1.2925095558166504\n",
      "Completed 300 out of 2209: 1.2567734718322754\n",
      "Completed 400 out of 2209: 1.2830185890197754\n",
      "Completed 500 out of 2209: 1.2646753787994385\n",
      "Completed 600 out of 2209: 1.2579948902130127\n",
      "Completed 700 out of 2209: 1.2687897682189941\n",
      "Completed 800 out of 2209: 1.2423028945922852\n",
      "Completed 900 out of 2209: 1.2686161994934082\n",
      "Completed 1000 out of 2209: 1.7898645401000977\n",
      "Completed 1100 out of 2209: 1.3413548469543457\n",
      "Completed 1200 out of 2209: 1.242807149887085\n",
      "Completed 1300 out of 2209: 1.2452261447906494\n",
      "Completed 1400 out of 2209: 1.282841444015503\n",
      "Completed 1500 out of 2209: 1.2598586082458496\n",
      "Completed 1600 out of 2209: 1.3161039352416992\n",
      "Completed 1700 out of 2209: 1.2881395816802979\n",
      "Completed 1800 out of 2209: 1.3163917064666748\n",
      "Completed 1900 out of 2209: 1.2446873188018799\n",
      "Completed 2000 out of 2209: 1.3242087364196777\n",
      "Completed 2100 out of 2209: 1.2446186542510986\n",
      "Completed 2200 out of 2209: 1.2337720394134521\n",
      "Done extracting sequences of length 20\n",
      "Completed 0 out of 2786: 0.005110502243041992\n",
      "Completed 100 out of 2786: 1.144348382949829\n",
      "Completed 200 out of 2786: 1.2305357456207275\n",
      "Completed 300 out of 2786: 1.2292087078094482\n",
      "Completed 400 out of 2786: 1.2498555183410645\n",
      "Completed 500 out of 2786: 1.364586353302002\n",
      "Completed 600 out of 2786: 1.320401668548584\n",
      "Completed 700 out of 2786: 1.398542881011963\n",
      "Completed 800 out of 2786: 1.6915562152862549\n",
      "Completed 900 out of 2786: 1.5200693607330322\n",
      "Completed 1000 out of 2786: 1.2843430042266846\n",
      "Completed 1100 out of 2786: 1.2915685176849365\n",
      "Completed 1200 out of 2786: 1.352412223815918\n",
      "Completed 1300 out of 2786: 1.308934211730957\n",
      "Completed 1400 out of 2786: 1.2835290431976318\n",
      "Completed 1500 out of 2786: 1.364790439605713\n",
      "Completed 1600 out of 2786: 1.3460485935211182\n",
      "Completed 1700 out of 2786: 1.4319753646850586\n",
      "Completed 1800 out of 2786: 1.253298282623291\n",
      "Completed 1900 out of 2786: 1.2712838649749756\n",
      "Completed 2000 out of 2786: 1.2884109020233154\n",
      "Completed 2100 out of 2786: 1.2528777122497559\n",
      "Completed 2200 out of 2786: 1.240652322769165\n",
      "Completed 2300 out of 2786: 1.2656669616699219\n",
      "Completed 2400 out of 2786: 1.244361400604248\n",
      "Completed 2500 out of 2786: 1.260118007659912\n",
      "Completed 2600 out of 2786: 1.2467453479766846\n",
      "Completed 2700 out of 2786: 1.2559893131256104\n",
      "Done extracting sequences of length 20\n",
      "Completed 0 out of 3218: 0.005375385284423828\n",
      "Completed 100 out of 3218: 1.162844181060791\n",
      "Completed 200 out of 3218: 1.288374662399292\n",
      "Completed 300 out of 3218: 1.2852075099945068\n",
      "Completed 400 out of 3218: 1.272768259048462\n",
      "Completed 500 out of 3218: 1.2603955268859863\n",
      "Completed 600 out of 3218: 1.265812635421753\n",
      "Completed 700 out of 3218: 1.257910966873169\n",
      "Completed 800 out of 3218: 1.2396841049194336\n",
      "Completed 900 out of 3218: 1.245309591293335\n",
      "Completed 1000 out of 3218: 1.2928504943847656\n",
      "Completed 1100 out of 3218: 1.2629237174987793\n",
      "Completed 1200 out of 3218: 1.2593283653259277\n",
      "Completed 1300 out of 3218: 1.283036708831787\n",
      "Completed 1400 out of 3218: 1.2407195568084717\n",
      "Completed 1500 out of 3218: 1.3446640968322754\n",
      "Completed 1600 out of 3218: 1.287377119064331\n",
      "Completed 1700 out of 3218: 1.261343002319336\n",
      "Completed 1800 out of 3218: 1.224313497543335\n",
      "Completed 1900 out of 3218: 1.2275526523590088\n",
      "Completed 2000 out of 3218: 1.2476410865783691\n",
      "Completed 2100 out of 3218: 1.251497745513916\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
