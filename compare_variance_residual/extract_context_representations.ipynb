{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import time as tm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import argparse\n",
    "import json\n",
    "import pandas as pd"
   ],
   "id": "2fe61507fb3cd56a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CACHE_DIR = \"../cache\"\n",
    "\n",
    "# Use GPU if possible\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ],
   "id": "be37331a9ea98231"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@torch.inference_mode()\n",
    "def get_model_layer_representations(args, text_array, word_ind_to_extract):\n",
    "    model, model_config, tokenizer = load_model(args)\n",
    "\n",
    "    # get the token embeddings\n",
    "    token_embeddings = []\n",
    "    for word in text_array:\n",
    "        current_token_embedding = get_model_token_embeddings([word], tokenizer, model)\n",
    "        token_embeddings.append(np.mean(current_token_embedding.detach().numpy(), 1))\n",
    "\n",
    "    # where to store layer-wise embeddings of particular length\n",
    "    words_layers_representations = {}\n",
    "    n_total_layers = model_config['num_layers']\n",
    "    for layer in range(n_total_layers):\n",
    "        words_layers_representations[layer] = []\n",
    "    words_layers_representations[-1] = token_embeddings\n",
    "\n",
    "    words_layers_representations = extract_context_representations(args, model, model_config, text_array, tokenizer,\n",
    "                                                                   word_ind_to_extract, words_layers_representations)\n",
    "    return words_layers_representations"
   ],
   "id": "47bd9c482e7a8777"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stories_path = \"../stimuli/formatted\"\n",
    "stories_files = {}\n",
    "\n",
    "for story in sorted(os.listdir(stories_path)):\n",
    "    story_path = os.path.join(stories_path, story)\n",
    "    words = open(story_path, 'r').read().strip().split('\\n')\n",
    "    embeddings = get_model_layer_representations(args, np.array(words), word_ind_to_extract)\n",
    "    stories_files[story] = embeddings\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Extract language model context representations from text')\n",
    "    parser.add_argument(\"--model\", help=\"natural language model used for extracting representations from the text\",\n",
    "                        type=str, default=\"bert-base\")\n",
    "    parser.add_argument(\"--sequence_length\", help=\"number of consequent words passed to the model as context\", type=int,\n",
    "                        default=20)\n",
    "    parser.add_argument(\"--output_file\", help=\"File name \", type=str, default=\"../bert_base20\")\n",
    "    args = parser.parse_args()"
   ],
   "id": "544ba05b7acc9633"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def extract_context_representations(args, model, model_config, text_array, tokenizer, word_ind_to_extract,\n",
    "                                    words_layers_representations):\n",
    "    # Before we've seen enough words to make up the seq_len\n",
    "    # Extract index 0 after supplying tokens 0 to 0, extract 1 after 0 to 1, 2 after 0 to 2, ... , 19 after 0 to 19\n",
    "    start_time = tm.time()\n",
    "    for seq_len in range(len(text_array)):\n",
    "        if seq_len < args.sequence_length:\n",
    "            word_seq = text_array[:seq_len + 1]\n",
    "            extract_index = seq_len\n",
    "        else:\n",
    "            word_seq = text_array[seq_len - args.sequence_length + 1:seq_len + 1]\n",
    "            extract_index = args.sequence_length + word_ind_to_extract if word_ind_to_extract < 0 else word_ind_to_extract\n",
    "\n",
    "        words_layers_representations = add_avrg_token_embedding_for_specific_word(word_seq, tokenizer, model,\n",
    "                                                                                  extract_index,\n",
    "                                                                                  words_layers_representations,\n",
    "                                                                                  model_config)\n",
    "\n",
    "        if seq_len % 100 == 0:\n",
    "            print('Completed {} out of {}: {}'.format(seq_len, len(text_array), tm.time() - start_time))\n",
    "            start_time = tm.time()\n",
    "    print('Done extracting sequences of length {}'.format(args.sequence_length))\n",
    "    return words_layers_representations\n",
    "\n",
    "\n",
    "def load_model(args):\n",
    "    with open('../text_model_config.json', 'r') as f:\n",
    "        model_config = json.load(f)[args.model]\n",
    "    model_hf_path = model_config['huggingface_hub']\n",
    "    print(model_config, model_hf_path, args.sequence_length)\n",
    "    model = AutoModel.from_pretrained(model_hf_path, cache_dir=CACHE_DIR).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_hf_path, cache_dir=CACHE_DIR)\n",
    "    model.eval()\n",
    "    return model, model_config, tokenizer\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_model_embeddings(words_in_array, tokenizer, model, model_config):\n",
    "    \"\"\"\n",
    "    extracts layer representations for all words in words_in_array\n",
    "    :param encoded_layers: list of tensors, length num layers. each tensor of dims num tokens by num dimensions in representation\n",
    "    :param word_ind_to_token_ind: dict that maps from index in words_in_array to index in array of tokens when words_in_array is tokenized,\n",
    "                      with keys: index of word, and values: array of indices of corresponding tokens when word is tokenized\n",
    "    \"\"\"\n",
    "    n_seq_tokens = 0\n",
    "    seq_tokens = []\n",
    "\n",
    "    word_ind_to_token_ind = {}  # dict that maps index of word in words_in_array to index of tokens in seq_tokens\n",
    "\n",
    "    for i, word in enumerate(words_in_array):\n",
    "        word_ind_to_token_ind[i] = []  # initialize token indices array for current word\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "        for token in word_tokens:\n",
    "            #             if token not in remove_chars:  # don't add any tokens that are in remove_chars\n",
    "            seq_tokens.append(token)\n",
    "            word_ind_to_token_ind[i].append(n_seq_tokens)\n",
    "            n_seq_tokens = n_seq_tokens + 1\n",
    "\n",
    "    # convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(seq_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
    "\n",
    "    if model_config['model_type'] == 'encoder' or model_config['model_type'] == 'decoder':\n",
    "        outputs = model(tokens_tensor, output_hidden_states=True)\n",
    "        hidden_states = outputs['hidden_states'][1:]  # This is a tuple: (layer1, layer2, ..., layer6)\n",
    "        all_layers_hidden_states = hidden_states\n",
    "    elif model_config['model_type'] == 'encoder-decoder':\n",
    "        outputs = model(tokens_tensor, decoder_input_ids=tokens_tensor, output_hidden_states=True)\n",
    "        encoder_hidden_states = outputs['encoder_hidden_states'][1:]  # This is a tuple: (layer1, layer2, ..., layer6)\n",
    "        decoder_hidden_states = outputs['decoder_hidden_states'][1:]\n",
    "        all_layers_hidden_states = encoder_hidden_states + decoder_hidden_states\n",
    "    else:\n",
    "        raise ValueError(\"model_type should be either encoder, decoder or encoder-decoder\")\n",
    "\n",
    "    return all_layers_hidden_states, word_ind_to_token_ind, None\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def add_word_model_embedding(model_dict, embeddings_to_add, token_inds_to_avrg, specific_layer=-1):\n",
    "    \"\"\"\n",
    "    add the embeddings for a specific word in the sequence\n",
    "\n",
    "    :param token_inds_to_avrg: indices of tokens in embeddings output to avrg\n",
    "    \"\"\"\n",
    "    if specific_layer >= 0:  # only add embeddings for one specified layer\n",
    "        layer_embedding = embeddings_to_add[specific_layer]\n",
    "        full_sequence_embedding = layer_embedding.cpu().detach().numpy()\n",
    "        model_dict[specific_layer].append(np.mean(full_sequence_embedding[0, token_inds_to_avrg, :], 0))\n",
    "    else:\n",
    "        for layer, layer_embedding in enumerate(embeddings_to_add):\n",
    "            full_sequence_embedding = layer_embedding.cpu().detach().numpy()\n",
    "            model_dict[layer].append(np.mean(full_sequence_embedding[0, token_inds_to_avrg, :],\n",
    "                                             0))  # avrg over all tokens for specified word\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def add_avrg_token_embedding_for_specific_word(word_seq, tokenizer, model, from_start_word_ind_to_extract, model_dict,\n",
    "                                               model_config):\n",
    "    \"\"\"\n",
    "    predicts representations for specific word in input word sequence, ad adds to existing layer-wise dictionary\n",
    "\n",
    "    :param word_seq: numpy array of words in input sequence\n",
    "    :param tokenizer: Auto tokenizer\n",
    "    :param model: Auto model\n",
    "    :param from_start_word_ind_to_extract: the index of the word whose features to extract, INDEXED FROM START OF WORD_SEQ\n",
    "    :param model_dict: where to save the extracted embeddings\n",
    "    \"\"\"\n",
    "    word_seq = list(word_seq)\n",
    "    all_sequence_embeddings, word_ind_to_token_ind, _ = predict_model_embeddings(word_seq, tokenizer, model,\n",
    "                                                                                 model_config)\n",
    "    token_inds_to_avrg = word_ind_to_token_ind[from_start_word_ind_to_extract]\n",
    "    model_dict = add_word_model_embedding(model_dict, all_sequence_embeddings, token_inds_to_avrg)\n",
    "\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_model_token_embeddings(words_in_array, tokenizer, model):\n",
    "    n_seq_tokens = 0\n",
    "    seq_tokens = []\n",
    "\n",
    "    word_ind_to_token_ind = {}  # dict that maps index of word in words_in_array to index of tokens in seq_tokens\n",
    "\n",
    "    for i, word in enumerate(words_in_array):\n",
    "        word_ind_to_token_ind[i] = []  # initialize token indices array for current word\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "        for token in word_tokens:\n",
    "            #             if token not in remove_chars:  # don't add any tokens that are in remove_chars\n",
    "            seq_tokens.append(token)\n",
    "            word_ind_to_token_ind[i].append(n_seq_tokens)\n",
    "            n_seq_tokens = n_seq_tokens + 1\n",
    "\n",
    "    # convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(seq_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
    "\n",
    "    input_embedding_module = model.base_model.get_input_embeddings()\n",
    "    token_embeddings = input_embedding_module(tokens_tensor.to(torch.long)).cpu()\n",
    "\n",
    "    return token_embeddings\n"
   ],
   "id": "f0ec1951ba94f66d"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
