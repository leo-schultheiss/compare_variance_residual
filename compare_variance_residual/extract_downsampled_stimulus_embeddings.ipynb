{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import time as tm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import json"
   ],
   "id": "4bb189fc655149da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CACHE_DIR = \"../cache\"\n",
    "DATA_DIR = \"../data\"\n",
    "grid_dir = \"../stimuli/grids\"\n",
    "trfiles_dir = \"../stimuli/trfiles\""
   ],
   "id": "61e66820776e056b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "def load_model(model_name):\n",
    "    with open('../text_model_config.json', 'r') as f:\n",
    "        model_config = json.load(f)[model_name]\n",
    "    model_hf_path = model_config['huggingface_hub']\n",
    "    model = AutoModel.from_pretrained(model_hf_path, cache_dir=CACHE_DIR).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_hf_path, cache_dir=CACHE_DIR)\n",
    "    model.eval()\n",
    "    return model, model_config, tokenizer\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_model_layer_representations(model_name, text_array):\n",
    "    model, model_config, tokenizer = load_model(model_name)\n",
    "\n",
    "    # get the token embeddings\n",
    "    token_embeddings = []\n",
    "    for word in text_array:\n",
    "        current_token_embedding = get_model_token_embeddings([word], tokenizer, model)\n",
    "        token_embeddings.append(np.mean(current_token_embedding.detach().numpy(), 1))\n",
    "\n",
    "    # layer-wise embeddings of particular length\n",
    "    words_layers_representations = {}\n",
    "    n_total_layers = model_config['num_layers']\n",
    "    for layer in range(n_total_layers):\n",
    "        words_layers_representations[layer] = []\n",
    "    words_layers_representations[-1] = token_embeddings\n",
    "\n",
    "    words_layers_representations = extract_context_representations(model, model_config, text_array, tokenizer,\n",
    "                                                                   words_layers_representations)\n",
    "    return words_layers_representations\n",
    "\n",
    "\n",
    "def extract_context_representations(model, model_config, text_array, tokenizer, words_layers_representations):\n",
    "    # Before we've seen enough words to make up the seq_len\n",
    "    # Extract index 0 after supplying tokens 0 to 0, extract 1 after 0 to 1, 2 after 0 to 2, ... , 19 after 0 to 19\n",
    "    start_time = tm.time()\n",
    "    for seq_len in range(len(text_array)):\n",
    "        if seq_len < sequence_length:\n",
    "            word_seq = text_array[:seq_len + 1]\n",
    "            extract_index = seq_len\n",
    "        else:\n",
    "            word_seq = text_array[seq_len - sequence_length + 1:seq_len + 1]\n",
    "            extract_index = sequence_length - 1\n",
    "\n",
    "        words_layers_representations = add_avrg_token_embedding_for_specific_word(word_seq, tokenizer, model,\n",
    "                                                                                  extract_index,\n",
    "                                                                                  words_layers_representations,\n",
    "                                                                                  model_config)\n",
    "\n",
    "        if seq_len % 100 == 0:\n",
    "            print('Completed {} out of {}: {}'.format(seq_len, len(text_array), tm.time() - start_time))\n",
    "            start_time = tm.time()\n",
    "    print(f'Done extracting sequences of length {sequence_length}')\n",
    "    return words_layers_representations\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_model_embeddings(words_in_array, tokenizer, model, model_config):\n",
    "    \"\"\"\n",
    "    extracts layer representations for all words in words_in_array\n",
    "    :param encoded_layers: list of tensors, length num layers. each tensor of dims num tokens by num dimensions in representation\n",
    "    :param word_ind_to_token_ind: dict that maps from index in words_in_array to index in array of tokens when words_in_array is tokenized,\n",
    "                      with keys: index of word, and values: array of indices of corresponding tokens when word is tokenized\n",
    "    \"\"\"\n",
    "    n_seq_tokens = 0\n",
    "    seq_tokens = []\n",
    "\n",
    "    word_ind_to_token_ind = {}  # dict that maps index of word in words_in_array to index of tokens in seq_tokens\n",
    "\n",
    "    for i, word in enumerate(words_in_array):\n",
    "        word_ind_to_token_ind[i] = []  # initialize token indices array for current word\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "        for token in word_tokens:\n",
    "            #             if token not in remove_chars:  # don't add any tokens that are in remove_chars\n",
    "            seq_tokens.append(token)\n",
    "            word_ind_to_token_ind[i].append(n_seq_tokens)\n",
    "            n_seq_tokens = n_seq_tokens + 1\n",
    "\n",
    "    # convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(seq_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
    "\n",
    "    if model_config['model_type'] == 'encoder' or model_config['model_type'] == 'decoder':\n",
    "        outputs = model(tokens_tensor, output_hidden_states=True)\n",
    "        hidden_states = outputs['hidden_states'][1:]  # This is a tuple: (layer1, layer2, ..., layer6)\n",
    "        all_layers_hidden_states = hidden_states\n",
    "    elif model_config['model_type'] == 'encoder-decoder':\n",
    "        outputs = model(tokens_tensor, decoder_input_ids=tokens_tensor, output_hidden_states=True)\n",
    "        encoder_hidden_states = outputs['encoder_hidden_states'][1:]  # This is a tuple: (layer1, layer2, ..., layer6)\n",
    "        decoder_hidden_states = outputs['decoder_hidden_states'][1:]\n",
    "        all_layers_hidden_states = encoder_hidden_states + decoder_hidden_states\n",
    "    else:\n",
    "        raise ValueError(\"model_type should be either encoder, decoder or encoder-decoder\")\n",
    "\n",
    "    return all_layers_hidden_states, word_ind_to_token_ind, None\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def add_word_model_embedding(model_dict, embeddings_to_add, token_inds_to_avrg, specific_layer=-1):\n",
    "    \"\"\"\n",
    "    add the embeddings for a specific word in the sequence\n",
    "\n",
    "    :param token_inds_to_avrg: indices of tokens in embeddings output to avrg\n",
    "    \"\"\"\n",
    "    if specific_layer >= 0:  # only add embeddings for one specified layer\n",
    "        layer_embedding = embeddings_to_add[specific_layer]\n",
    "        full_sequence_embedding = layer_embedding.cpu().detach().numpy()\n",
    "        model_dict[specific_layer].append(np.mean(full_sequence_embedding[0, token_inds_to_avrg, :], 0))\n",
    "    else:\n",
    "        for layer, layer_embedding in enumerate(embeddings_to_add):\n",
    "            full_sequence_embedding = layer_embedding.cpu().detach().numpy()\n",
    "            model_dict[layer].append(np.mean(full_sequence_embedding[0, token_inds_to_avrg, :],\n",
    "                                             0))  # avrg over all tokens for specified word\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def add_avrg_token_embedding_for_specific_word(word_seq, tokenizer, model, from_start_word_ind_to_extract, model_dict,\n",
    "                                               model_config):\n",
    "    \"\"\"\n",
    "    predicts representations for specific word in input word sequence, ad adds to existing layer-wise dictionary\n",
    "\n",
    "    :param word_seq: numpy array of words in input sequence\n",
    "    :param tokenizer: Auto tokenizer\n",
    "    :param model: Auto model\n",
    "    :param from_start_word_ind_to_extract: the index of the word whose features to extract, INDEXED FROM START OF WORD_SEQ\n",
    "    :param model_dict: where to save the extracted embeddings\n",
    "    \"\"\"\n",
    "    word_seq = list(word_seq)\n",
    "    all_sequence_embeddings, word_ind_to_token_ind, _ = predict_model_embeddings(word_seq, tokenizer, model,\n",
    "                                                                                 model_config)\n",
    "    token_inds_to_avrg = word_ind_to_token_ind[from_start_word_ind_to_extract]\n",
    "    model_dict = add_word_model_embedding(model_dict, all_sequence_embeddings, token_inds_to_avrg)\n",
    "\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_model_token_embeddings(words_in_array, tokenizer, model):\n",
    "    n_seq_tokens = 0\n",
    "    seq_tokens = []\n",
    "\n",
    "    word_ind_to_token_ind = {}  # dict that maps index of word in words_in_array to index of tokens in seq_tokens\n",
    "\n",
    "    for i, word in enumerate(words_in_array):\n",
    "        word_ind_to_token_ind[i] = []  # initialize token indices array for current word\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "        for token in word_tokens:\n",
    "            #             if token not in remove_chars:  # don't add any tokens that are in remove_chars\n",
    "            seq_tokens.append(token)\n",
    "            word_ind_to_token_ind[i].append(n_seq_tokens)\n",
    "            n_seq_tokens = n_seq_tokens + 1\n",
    "\n",
    "    # convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(seq_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
    "\n",
    "    input_embedding_module = model.base_model.get_input_embeddings()\n",
    "    token_embeddings = input_embedding_module(tokens_tensor.to(torch.long)).cpu()\n",
    "\n",
    "    return token_embeddings"
   ],
   "id": "7a0ac41a971c977"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Extract Context Representations",
   "id": "b7972e9c0450316c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stories_path = \"../stimuli/formatted\"\n",
    "sequence_length = 20\n",
    "model_name = \"bert-base\"\n",
    "representations_file = f\"../{model_name}{sequence_length}.npy\""
   ],
   "id": "d0d6150bb6f7fc7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if os.path.exists(representations_file):\n",
    "    print(\"File already exists, loading embeddings\")\n",
    "    stories_embeddings = np.load(representations_file, allow_pickle=True)\n",
    "else:\n",
    "    stories_embeddings = {}\n",
    "    for story in sorted(os.listdir(stories_path)):\n",
    "        print(story)\n",
    "        story_path = os.path.join(stories_path, story)\n",
    "        words = open(story_path, 'r').read().strip().split('\\n')\n",
    "        embeddings = get_model_layer_representations(model_name, np.array(words))\n",
    "        stories_embeddings[story] = embeddings\n",
    "\n",
    "    np.save(representations_file, stories_embeddings)\n",
    "print(\"Done\")"
   ],
   "id": "ef995caec496e1c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From the extracted embeddings, we will now downsample the stimuli to match the length of the semantic model.",
   "id": "3466dd87d9f355d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "downsampled_semanticseqs_file = f\"../{model_name}{sequence_length}_downsampled.npy\"\n",
    "if os.path.exists(downsampled_semanticseqs_file):\n",
    "    print(\"File already exists\")\n",
    "    exit()"
   ],
   "id": "69cabe7ef4a79176"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_story_names = ['alternateithicatom', 'avatar', 'howtodraw', 'legacy',\n",
    "                        'life', 'myfirstdaywiththeyankees', 'naked',\n",
    "                        'odetostepfather', 'souls', 'undertheinfluence']\n",
    "testing_story_names = ['wheretheressmoke']\n",
    "all_story_names = training_story_names + testing_story_names\n",
    "print(all_story_names)"
   ],
   "id": "39d3fd03a0507c10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from compare_variance_residual.stimuli_utils.textgrid import TextGrid\n",
    "\n",
    "grids = {}\n",
    "for story in all_story_names:\n",
    "    print(story)\n",
    "    gridfile = [os.path.join(grid_dir, gf) for gf in os.listdir(grid_dir) if gf.startswith(story)][0]\n",
    "    with open(gridfile) as f:\n",
    "        grids[story] = TextGrid(f.read())"
   ],
   "id": "185718c39cb0403a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from compare_variance_residual.stimuli_utils.trfile import TRFile\n",
    "\n",
    "trfiles = dict()\n",
    "\n",
    "for story in all_story_names:\n",
    "    try:\n",
    "        trf = TRFile(os.path.join(trfiles_dir, \"%s.report\" % story))\n",
    "        trfiles[story] = [trf]\n",
    "    except Exception as e:\n",
    "        print(e)"
   ],
   "id": "145e6f6c83ed5754"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from compare_variance_residual.stimuli_utils.SemtanticModel import SemanticModel\n",
    "from ridge_utils.dsutils import make_word_ds\n",
    "\n",
    "# Make word and phoneme datasequences\n",
    "wordseqs = make_word_ds(grids, trfiles)  # dictionary of {storyname : word DataSequence}\n",
    "eng1000 = SemanticModel.load(os.path.join(DATA_DIR, \"english1000sm.hf5\"))"
   ],
   "id": "b3be1a85befdb61b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ridge_utils.dsutils import make_semantic_model\n",
    "\n",
    "semanticseqs = dict()  # dictionary to hold projected stimuli {story name : projected DataSequence}\n",
    "for story in all_story_names:\n",
    "    semanticseqs[story] = make_semantic_model(wordseqs[story], [eng1000], [985])\n",
    "# story_filenames = ['alternateithicatom', 'avatar', 'howtodraw', 'legacy',\n",
    "#                    'life', 'myfirstdaywiththeyankees', 'naked',\n",
    "#                    'odetostepfather', 'souls', 'undertheinfluence', 'wheretheressmoke']\n",
    "# semanticseqs = dict()\n",
    "# for i in np.arange(len(all_story_names)):\n",
    "#     temp = make_semantic_model(wordseqs[all_story_names[i]], [eng1000], [985])\n",
    "#     temp.data = np.nan_to_num(stories_embeddings.item()[story_filenames[i]][layer])\n",
    "#     semanticseqs[all_story_names[i]] = temp"
   ],
   "id": "86bbc2639b1efa77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Downsample stimuli",
   "id": "317102b3a80dac56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "interptype = \"lanczos\"  # filter type\n",
    "window = 3  # number of lobes in Lanczos filter\n",
    "downsampled_semanticseqs = dict()  # dictionary to hold downsampled stimuli\n",
    "for story in all_story_names:\n",
    "    downsampled_semanticseqs[story] = semanticseqs[story].chunksums(interptype, window=window)\n",
    "\n",
    "np.save(downsampled_semanticseqs_file, downsampled_semanticseqs)"
   ],
   "id": "1aaf41247ab8c432"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
