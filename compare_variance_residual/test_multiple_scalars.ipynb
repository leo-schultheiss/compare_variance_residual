{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:44:46.815467Z",
     "start_time": "2025-02-12T10:44:46.722027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os.path\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from himalaya.backend import set_backend\n",
    "\n",
    "from compare_variance_residual.plotting import plot_variance_partitioning_results, plot_residual_method_results, plot_variance_vs_residual_joint\n",
    "from compare_variance_residual.residual import residual_method\n",
    "from compare_variance_residual.simulation import generate_dataset\n",
    "from compare_variance_residual.variance_partitioning import variance_partitioning"
   ],
   "id": "5cabe13459a7b132",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-12T10:44:46.928966Z",
     "start_time": "2025-02-12T10:44:46.818663Z"
    }
   },
   "source": [
    "backend = set_backend(\"cupy\", on_error=\"warn\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(42)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:44:47.076477Z",
     "start_time": "2025-02-12T10:44:47.074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_targets = 10000\n",
    "n_samples_train = 10000\n",
    "n_samples_test = 1000\n",
    "n_samples = n_samples_train + n_samples_test\n",
    "noise_scalar = 0.1\n",
    "\n",
    "cv = 20\n",
    "alphas = np.logspace(-4, 4, 10)"
   ],
   "id": "cf1955458c8b8a2c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:44:47.124937Z",
     "start_time": "2025-02-12T10:44:47.122115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = os.path.join(\"results\", f\"targets={n_targets}\", f\"samples={n_samples}\", f\"noise={noise_scalar}\", f\"cv={cv}\",\n",
    "                    f\"alphas={alphas.min()},{alphas.max()},{len(alphas)}\", \"varying scalars\")"
   ],
   "id": "247c0e6a3a9e22ce",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:44:47.170702Z",
     "start_time": "2025-02-12T10:44:47.168580Z"
    }
   },
   "cell_type": "code",
   "source": "OVERWRITE = False",
   "id": "c6b117b224a0ecf6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing proportions of contribution",
   "id": "5dc32162b056bc86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:44:47.217274Z",
     "start_time": "2025-02-12T10:44:47.214966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_list = [\n",
    "    100,  # shared\n",
    "    100,  # unique 0\n",
    "    100,  # unique 1\n",
    "]\n",
    "varying_scalar = np.linspace(0, 1, 10)\n",
    "other_scalars = np.linspace(1, 0, 10) / 2"
   ],
   "id": "55550e459b2c7761",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test different $a_\\mathbf{A}$",
   "id": "f13fdb8c0e7714a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:44:47.271554Z",
     "start_time": "2025-02-12T10:44:47.266063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "experiment_path = os.path.join(path, \"shared contribution\")\n",
    "os.makedirs(experiment_path, exist_ok=True)\n",
    "scalars_list = [[varying, other, other] for varying, other in zip(varying_scalar, other_scalars)]\n",
    "scalars_list"
   ],
   "id": "782210d20d2fae89",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[np.float64(0.0), np.float64(0.5), np.float64(0.5)],\n",
       " [np.float64(0.1111111111111111),\n",
       "  np.float64(0.4444444444444444),\n",
       "  np.float64(0.4444444444444444)],\n",
       " [np.float64(0.2222222222222222),\n",
       "  np.float64(0.3888888888888889),\n",
       "  np.float64(0.3888888888888889)],\n",
       " [np.float64(0.3333333333333333),\n",
       "  np.float64(0.33333333333333337),\n",
       "  np.float64(0.33333333333333337)],\n",
       " [np.float64(0.4444444444444444),\n",
       "  np.float64(0.2777777777777778),\n",
       "  np.float64(0.2777777777777778)],\n",
       " [np.float64(0.5555555555555556),\n",
       "  np.float64(0.2222222222222222),\n",
       "  np.float64(0.2222222222222222)],\n",
       " [np.float64(0.6666666666666666),\n",
       "  np.float64(0.16666666666666669),\n",
       "  np.float64(0.16666666666666669)],\n",
       " [np.float64(0.7777777777777777),\n",
       "  np.float64(0.11111111111111116),\n",
       "  np.float64(0.11111111111111116)],\n",
       " [np.float64(0.8888888888888888),\n",
       "  np.float64(0.05555555555555558),\n",
       "  np.float64(0.05555555555555558)],\n",
       " [np.float64(1.0), np.float64(0.0), np.float64(0.0)]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-12T09:46:57.250308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for scalars in scalars_list:\n",
    "    print(scalars)\n",
    "    csv_path = os.path.join(experiment_path, f\"scores_{scalars}.csv\")\n",
    "    if not OVERWRITE and os.path.exists(csv_path):\n",
    "        print(\"skipping, already exists\")\n",
    "        continue\n",
    "    Xs, Y = generate_dataset(d_list, scalars, n_targets, n_samples, noise_scalar)\n",
    "    x1_score, x2_score, joint_score, x1_and_x2_score, vp_x1_unique_score, vp_x2_unique_score = variance_partitioning(\n",
    "        Xs, Y, n_samples_train, alphas, cv)\n",
    "    print(\"variance partitioning done\")\n",
    "    _, _, x2_to_x1_score, x1_to_x2_score, rm_x1_unique_score, rm_x2_unique_score = residual_method(\n",
    "        Xs, Y, n_samples_train, alphas, cv)\n",
    "    print(\"residual method done\")\n",
    "    scores = pd.DataFrame()\n",
    "    scores[r\"$f(X_1\\cup X_2)\\approx Y$\"] = joint_score\n",
    "    scores[r\"$g(X_1)\\approx Y$\"] = x1_score\n",
    "    scores[r\"$h(X_2)\\approx Y$\"] = x2_score\n",
    "    scores[r\"$i(X_1)\\cap h(X_2)$\"] = x1_and_x2_score\n",
    "    scores[r\"$j(X_1) \\setminus h(X_2)$\"] = vp_x1_unique_score\n",
    "    scores[r\"$k(X_2) \\setminus g(X_1)$\"] = vp_x2_unique_score\n",
    "\n",
    "    scores[r\"$l(X_2)\\approx X_1$\"] = np.concatenate(\n",
    "        [x2_to_x1_score, np.full(len(x1_score) - len(x2_to_x1_score), np.nan)])\n",
    "    scores[r\"$m(X_1)\\approx X_2$\"] = np.concatenate(\n",
    "        [x1_to_x2_score, np.full(len(x2_score) - len(x1_to_x2_score), np.nan)])\n",
    "    scores[r\"$n(X_1 - l(X_2)) \\approx Y$\"] = rm_x1_unique_score\n",
    "    scores[r\"$o(X_2 - m(X_1)) \\approx Y$\"] = rm_x2_unique_score\n",
    "    print(scores.head())\n",
    "\n",
    "    scores.to_csv(csv_path, index=False)"
   ],
   "id": "662c048c3e7d8c4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.0), np.float64(0.5), np.float64(0.5)]\n",
      "skipping, already exists\n",
      "[np.float64(0.1111111111111111), np.float64(0.4444444444444444), np.float64(0.4444444444444444)]\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for scalars in scalars_list:\n",
    "    scores_path = os.path.join(experiment_path, f\"scores_{scalars}.csv\")\n",
    "    scores = pd.read_csv(scores_path)\n",
    "    # take first 6 columns to get variance partitioning scores\n",
    "    vp_scores = scores.iloc[:, :6]\n",
    "    plot_variance_partitioning_results(scalars, vp_scores)\n",
    "    # take full scores and last 4 columns to get residual scores\n",
    "    rm_scores = scores.iloc[:, 1:3]\n",
    "    rm_scores = pd.concat([rm_scores, scores.iloc[:, 6:]])\n",
    "    print(rm_scores.head())\n",
    "    plot_residual_method_results(scalars, d_list, rm_scores)\n",
    "\n",
    "    vp_x1_unique_error = scores.iloc[:, 4] - scalars[1]\n",
    "    vp_x2_unique_error = scores.iloc[:, 5] - scalars[2]\n",
    "    residual_x1_unique_error = scores.iloc[:, 8] - scalars[1]\n",
    "    residual_x2_unique_error = scores.iloc[:, 9] - scalars[2]\n",
    "\n",
    "    # Columns: voxel index, error, method, feature space\n",
    "    voxel_index = np.arange(len(vp_x1_unique_error))\n",
    "    error = pd.DataFrame({\n",
    "        \"Voxel Index\": np.concatenate(\n",
    "            [voxel_index, voxel_index]),\n",
    "        \"VP Error\": np.concatenate(\n",
    "            [vp_x1_unique_error, vp_x2_unique_error]),\n",
    "        \"Residual Error\": np.concatenate([residual_x1_unique_error, residual_x2_unique_error]),\n",
    "        \"Feature Space\": [r\"$X_1$\"] * len(vp_x1_unique_error) + [r\"$X_2$\"] * len(vp_x2_unique_error)\n",
    "    })\n",
    "\n",
    "    plot_variance_vs_residual_joint(scalars, error)"
   ],
   "id": "bc36c400283c9e18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test different $a_\\mathbf{B}$",
   "id": "d0044295d36d9729"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scalars_list = [[other, varying, other] for varying, other in zip(varying_scalar, other_scalars)]\n",
    "scalars_list"
   ],
   "id": "b83708b55da08dac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for scalars in scalars_list:\n",
    "    print(scalars)\n",
    "    csv_path = os.path.join(experiment_path, f\"scores_{scalars}.csv\")\n",
    "    if not OVERWRITE and os.path.exists(csv_path):\n",
    "        print(\"skipping, already exists\")\n",
    "        continue\n",
    "    Xs, Y = generate_dataset(d_list, scalars, n_targets, n_samples, noise_scalar)\n",
    "    x1_score, x2_score, joint_score, x1_and_x2_score, vp_x1_unique_score, vp_x2_unique_score = variance_partitioning(\n",
    "        Xs, Y, n_samples_train, alphas, cv)\n",
    "    print(\"variance partitioning done\")\n",
    "    _, _, x2_to_x1_score, x1_to_x2_score, rm_x1_unique_score, rm_x2_unique_score = residual_method(\n",
    "        Xs, Y, n_samples_train, alphas, cv)\n",
    "    print(\"residual method done\")\n",
    "    scores = pd.DataFrame()\n",
    "    scores[r\"$f(X_1\\cup X_2)\\approx Y$\"] = joint_score\n",
    "    scores[r\"$g(X_1)\\approx Y$\"] = x1_score\n",
    "    scores[r\"$h(X_2)\\approx Y$\"] = x2_score\n",
    "    scores[r\"$i(X_1)\\cap h(X_2)$\"] = x1_and_x2_score\n",
    "    scores[r\"$j(X_1) \\setminus h(X_2)$\"] = vp_x1_unique_score\n",
    "    scores[r\"$k(X_2) \\setminus g(X_1)$\"] = vp_x2_unique_score\n",
    "\n",
    "    scores[r\"$l(X_2)\\approx X_1$\"] = np.concatenate(\n",
    "        [x2_to_x1_score, np.full(len(x1_score) - len(x2_to_x1_score), np.nan)])\n",
    "    scores[r\"$m(X_1)\\approx X_2$\"] = np.concatenate(\n",
    "        [x1_to_x2_score, np.full(len(x2_score) - len(x1_to_x2_score), np.nan)])\n",
    "    scores[r\"$n(X_1 - l(X_2)) \\approx Y$\"] = rm_x1_unique_score\n",
    "    scores[r\"$o(X_2 - m(X_1)) \\approx Y$\"] = rm_x2_unique_score\n",
    "    print(scores.head())\n",
    "\n",
    "    scores.to_csv(csv_path, index=False)"
   ],
   "id": "8064afdaa00a7dc5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
