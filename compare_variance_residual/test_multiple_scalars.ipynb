{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os.path\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from himalaya.backend import set_backend\n",
    "\n",
    "from compare_variance_residual.plotting import plot_variance_partitioning_results, plot_residual_method_results, plot_variance_vs_residual_joint\n",
    "from compare_variance_residual.residual import residual_method\n",
    "from compare_variance_residual.simulation import generate_dataset\n",
    "from compare_variance_residual.variance_partitioning import variance_partitioning"
   ],
   "id": "9bf65b6335779e2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "backend = set_backend(\"cupy\", on_error=\"warn\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(42)"
   ],
   "id": "eb3204643d477652"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_targets = 10000\n",
    "n_samples_train = 10000\n",
    "n_samples_test = 1000\n",
    "n_samples = n_samples_train + n_samples_test\n",
    "noise_scalar = 0.1\n",
    "\n",
    "cv = 20\n",
    "alphas = np.logspace(-4, 4, 10)"
   ],
   "id": "d08c488538f933ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "path = os.path.join(\"results\", f\"targets={n_targets}\", f\"samples={n_samples}\", f\"noise={noise_scalar}\", f\"cv={cv}\",\n",
    "                    f\"alphas={alphas.min()},{alphas.max()},{len(alphas)}\", \"varying scalars\")"
   ],
   "id": "762d884deaf78e4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "OVERWRITE = False",
   "id": "c594a101642392f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing proportions of contribution",
   "id": "ab9da8b27885e7db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "d_list = [\n",
    "    100,  # shared\n",
    "    100,  # unique 0\n",
    "    100,  # unique 1\n",
    "]\n",
    "varying_scalar = np.linspace(0, 1, 10)\n",
    "other_scalars = np.linspace(1, 0, 10) / 2"
   ],
   "id": "3787b5b02fa9561a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test different $a_\\mathbf{A}$",
   "id": "5a428c875f9ac789"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "experiment_path = os.path.join(path, \"shared contribution\")\n",
    "os.makedirs(experiment_path, exist_ok=True)\n",
    "scalars_list = [[varying, other, other] for varying, other in zip(varying_scalar, other_scalars)]\n",
    "scalars_list"
   ],
   "id": "f4989894f193b6e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for scalars in scalars_list:\n",
    "    print(scalars)\n",
    "    csv_path = os.path.join(experiment_path, f\"scores_{scalars}.csv\")\n",
    "    if not OVERWRITE and os.path.exists(csv_path):\n",
    "        print(\"skipping, already exists\")\n",
    "        continue\n",
    "    Xs, Y = generate_dataset(d_list, scalars, n_targets, n_samples, noise_scalar)\n",
    "    x1_score, x2_score, joint_score, x1_and_x2_score, vp_x1_unique_score, vp_x2_unique_score = variance_partitioning(\n",
    "        Xs, Y, n_samples_train, alphas, cv)\n",
    "    print(\"variance partitioning done\")\n",
    "    x2_to_x1_score, x1_to_x2_score, rm_x1_unique_score, rm_x2_unique_score = residual_method(\n",
    "        Xs, Y, n_samples_train, alphas, cv)\n",
    "    print(\"residual method done\")\n",
    "    scores = pd.DataFrame()\n",
    "    scores[r\"$f(X_1\\cup X_2)\\approx Y$\"] = joint_score\n",
    "    scores[r\"$g(X_1)\\approx Y$\"] = x1_score\n",
    "    scores[r\"$h(X_2)\\approx Y$\"] = x2_score\n",
    "    scores[r\"$i(X_1)\\cap h(X_2)$\"] = x1_and_x2_score\n",
    "    scores[r\"$j(X_1) \\setminus h(X_2)$\"] = vp_x1_unique_score\n",
    "    scores[r\"$k(X_2) \\setminus g(X_1)$\"] = vp_x2_unique_score\n",
    "\n",
    "    scores[r\"$l(X_2)\\approx X_1$\"] = np.concatenate(\n",
    "        [x2_to_x1_score, np.full(len(x1_score) - len(x2_to_x1_score), np.nan)])\n",
    "    scores[r\"$m(X_1)\\approx X_2$\"] = np.concatenate(\n",
    "        [x1_to_x2_score, np.full(len(x2_score) - len(x1_to_x2_score), np.nan)])\n",
    "    scores[r\"$n(X_1 - l(X_2)) \\approx Y$\"] = rm_x1_unique_score\n",
    "    scores[r\"$o(X_2 - m(X_1)) \\approx Y$\"] = rm_x2_unique_score\n",
    "    print(scores.head())\n",
    "\n",
    "    scores.to_csv(csv_path, index=False)"
   ],
   "id": "661dbb322a4f948e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for scalars in scalars_list:\n",
    "    scores_path = os.path.join(experiment_path, f\"scores_{scalars}.csv\")\n",
    "    scores = pd.read_csv(scores_path)\n",
    "    # take first 6 columns to get variance partitioning scores\n",
    "    vp_scores = scores.iloc[:, :6]\n",
    "    plot_variance_partitioning_results(scalars, vp_scores)\n",
    "    # take full scores and last 4 columns to get residual scores\n",
    "    rm_scores = scores.iloc[:, 1:3]\n",
    "    rm_scores = pd.concat([rm_scores, scores.iloc[:, 6:]])\n",
    "    print(rm_scores.head())\n",
    "    plot_residual_method_results(scalars, d_list, rm_scores)\n",
    "\n",
    "    vp_x1_unique_error = scores.iloc[:, 4] - scalars[1]\n",
    "    vp_x2_unique_error = scores.iloc[:, 5] - scalars[2]\n",
    "    residual_x1_unique_error = scores.iloc[:, 8] - scalars[1]\n",
    "    residual_x2_unique_error = scores.iloc[:, 9] - scalars[2]\n",
    "\n",
    "    # Columns: voxel index, error, method, feature space\n",
    "    voxel_index = np.arange(len(vp_x1_unique_error))\n",
    "    error = pd.DataFrame({\n",
    "        \"Voxel Index\": np.concatenate(\n",
    "            [voxel_index, voxel_index]),\n",
    "        \"VP Error\": np.concatenate(\n",
    "            [vp_x1_unique_error, vp_x2_unique_error]),\n",
    "        \"Residual Error\": np.concatenate([residual_x1_unique_error, residual_x2_unique_error]),\n",
    "        \"Feature Space\": [r\"$X_1$\"] * len(vp_x1_unique_error) + [r\"$X_2$\"] * len(vp_x2_unique_error)\n",
    "    })\n",
    "\n",
    "    plot_variance_vs_residual_joint(scalars, error)"
   ],
   "id": "4ee1ccbbb714e799"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test different $a_\\mathbf{B}$",
   "id": "b12e4c923b9aeeec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scalars_list = [[other, varying, other] for varying, other in zip(varying_scalar, other_scalars)]\n",
    "scalars_list"
   ],
   "id": "3ca5e20c4bb5c806"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for scalars in scalars_list:\n",
    "    print(scalars)\n",
    "    csv_path = os.path.join(experiment_path, f\"scores_{scalars}.csv\")\n",
    "    if not OVERWRITE and os.path.exists(csv_path):\n",
    "        print(\"skipping, already exists\")\n",
    "        continue\n",
    "    Xs, Y = generate_dataset(d_list, scalars, n_targets, n_samples, noise_scalar)\n",
    "    x1_score, x2_score, joint_score, x1_and_x2_score, vp_x1_unique_score, vp_x2_unique_score = variance_partitioning(\n",
    "        Xs, Y, n_samples_train, alphas, cv)\n",
    "    print(\"variance partitioning done\")\n",
    "    x2_to_x1_score, x1_to_x2_score, rm_x1_unique_score, rm_x2_unique_score = residual_method(\n",
    "        Xs, Y, n_samples_train, alphas, cv)\n",
    "    print(\"residual method done\")\n",
    "    scores = pd.DataFrame()\n",
    "    scores[r\"$f(X_1\\cup X_2)\\approx Y$\"] = joint_score\n",
    "    scores[r\"$g(X_1)\\approx Y$\"] = x1_score\n",
    "    scores[r\"$h(X_2)\\approx Y$\"] = x2_score\n",
    "    scores[r\"$i(X_1)\\cap h(X_2)$\"] = x1_and_x2_score\n",
    "    scores[r\"$j(X_1) \\setminus h(X_2)$\"] = vp_x1_unique_score\n",
    "    scores[r\"$k(X_2) \\setminus g(X_1)$\"] = vp_x2_unique_score\n",
    "\n",
    "    scores[r\"$l(X_2)\\approx X_1$\"] = np.concatenate(\n",
    "        [x2_to_x1_score, np.full(len(x1_score) - len(x2_to_x1_score), np.nan)])\n",
    "    scores[r\"$m(X_1)\\approx X_2$\"] = np.concatenate(\n",
    "        [x1_to_x2_score, np.full(len(x2_score) - len(x1_to_x2_score), np.nan)])\n",
    "    scores[r\"$n(X_1 - l(X_2)) \\approx Y$\"] = rm_x1_unique_score\n",
    "    scores[r\"$o(X_2 - m(X_1)) \\approx Y$\"] = rm_x2_unique_score\n",
    "    print(scores.head())\n",
    "\n",
    "    scores.to_csv(csv_path, index=False)"
   ],
   "id": "6b2c6bfb5d778fd2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
