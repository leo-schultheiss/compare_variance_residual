{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T12:31:33.550595Z",
     "start_time": "2025-02-04T12:31:33.406730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "from importlib import reload\n",
    "\n",
    "import himalaya.scoring\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from himalaya.backend import set_backend\n",
    "from matplotlib import axes\n",
    "\n",
    "from compare_variance_residual.simulation import generate_dataset\n",
    "\n",
    "set_backend(\"cupy\", on_error=\"warn\")\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "529c318b8f13ea16",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'himalaya'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mimportlib\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m reload\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mhimalaya\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mscoring\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'himalaya'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "plt.style.use(\"nord\")"
   ],
   "id": "72339f2f171593e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T12:31:33.849447Z",
     "start_time": "2025-02-04T12:31:33.814314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generic_errorbar_plot(means, variances, labels, horizontal_positions, title, suptitle, ylabel, xlabel,\n",
    "                          n_samples_train, n_samples_test, ax: axes.Axes = None, fig=None):\n",
    "    \"\"\"Generic plot function for shared parameters between vp_errorbar and rm_errorbar.\"\"\"\n",
    "\n",
    "    if ax is None or fig is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    yerr = [np.sqrt(var) / np.sqrt(n_samples_test) for var in variances]\n",
    "    colors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\", \"tab:brown\"]\n",
    "\n",
    "    # Create bar plot\n",
    "    bars = ax.bar(range(len(means)), means, color=colors[:len(means)], yerr=yerr, capsize=5, ecolor=\"black\")\n",
    "    ax.bar_label(bars, fmt='{:,.2f}', label_type='center')\n",
    "    ax.set_xticks(range(len(means)), labels=labels)\n",
    "\n",
    "    # Function to plot horizontal lines\n",
    "    def plot_horizontal_line(start, end, y_value, label, color):\n",
    "        ax.plot([start, end], [y_value, y_value], linestyle='--', label=label, color=color)\n",
    "\n",
    "    # Plot horizontal lines\n",
    "    for i, (y_value, label, start, end) in enumerate(horizontal_positions):\n",
    "        plot_horizontal_line(start, end, y_value, label, colors[i % len(colors)])\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    ylims = ax.get_ylim()\n",
    "    ax.set_ylim(ylims[0], 1.01)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(xlabel)\n",
    "\n",
    "    fig.suptitle(suptitle)\n",
    "    ax.set_title(title)\n",
    "    return ax, fig"
   ],
   "id": "72bd87122cb8b060",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'axes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgeneric_errorbar_plot\u001B[39m(means, variances, labels, horizontal_positions, title, suptitle, ylabel, xlabel,\n\u001B[0;32m----> 2\u001B[0m                           n_samples_train, n_samples_test, ax: \u001B[43maxes\u001B[49m\u001B[38;5;241m.\u001B[39mAxes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, fig\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m      3\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Generic plot function for shared parameters between vp_errorbar and rm_errorbar.\"\"\"\u001B[39;00m\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ax \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m fig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'axes' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "feature_space_dimensions = [\n",
    "    100,  # shared\n",
    "    100,  # unique 0\n",
    "    100,  # unique 1\n",
    "]\n",
    "scalars = [\n",
    "    1 / 3, 1 / 3, 1 / 3\n",
    "]\n",
    "n_targets = 1000\n",
    "n_samples_train = 10000\n",
    "n_samples_test = 1000\n",
    "noise_scalar = 0.1\n",
    "\n",
    "cv = 20\n",
    "alphas = np.logspace(-4, 4, 10)"
   ],
   "id": "cd6bbd72f85463dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "(Xs_train, Xs_test, Y_train, Y_test) = generate_dataset(feature_space_dimensions, scalars, n_targets, n_samples_train,\n",
    "                                                        n_samples_test, noise_scalar)"
   ],
   "id": "3599f619506dac2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Variance Partitioning",
   "id": "38925c28bd0a0a32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def vp_errorbar(score_0, score_1, joint_score, shared, x0_unique, x1_unique, feature_space_dimensions, scalars,\n",
    "                n_targets, n_samples_train, n_samples_test, noise_scalar, ax: axes.Axes = None, fig=None):\n",
    "    # Precompute means and variances\n",
    "    scores = [score_0, score_1, joint_score, shared, x0_unique, x1_unique]\n",
    "    means = [s.mean() for s in scores]\n",
    "    variances = [s.var() for s in scores]\n",
    "\n",
    "    bar_names = [r\"$X_0$\", r\"$X_1$\", r\"$X_0 \\cup X_1$\", r\"$X_0 \\cap X_1$\", r\"$X_0 \\setminus X_1$\",\n",
    "                 r\"$X_1 \\setminus X_0$\"]\n",
    "\n",
    "    # Horizontal line positions\n",
    "    horizontal_positions = [\n",
    "        (scalars[0] + scalars[1], r\"$a_S + a_{U_0}$\", -0.5, 0.5),\n",
    "        (scalars[0] + scalars[2], r\"$a_S + a_{U_1}$\", 0.5, 1.5),\n",
    "        (sum(scalars), r\"$a_S + a_{U_0} + a_{U_1}$\", 1.5, 2.5),\n",
    "        (scalars[0], r\"$a_S$\", 2.5, 3.5),\n",
    "        (scalars[1], r\"$a_{U_0}$\", 3.5, 4.5),\n",
    "        (scalars[2], r\"$a_{U_1}$\", 4.5, 5.5)\n",
    "    ]\n",
    "\n",
    "    # Call the generic function\n",
    "    generic_errorbar_plot(\n",
    "        means=means,\n",
    "        variances=variances,\n",
    "        labels=bar_names,\n",
    "        horizontal_positions=horizontal_positions,\n",
    "        title=fr\"$a_S$: {scalars[0]:.2f}, $a_{{U_0}}$: {scalars[1]:.2f}, $a_{{U_1}}$: {scalars[2]:.2f}, $|S|$: {feature_space_dimensions[0]}, $|U_0|$: {feature_space_dimensions[1]}, $|U_1|$: {feature_space_dimensions[2]}, $a_E$: {noise_scalar}\",\n",
    "        suptitle=\"Variance partitioning\",\n",
    "        ylabel=r\"Average Variance Explained ($r^2$)\",\n",
    "        xlabel=\"Feature space\",\n",
    "        n_samples_test=n_samples_test,\n",
    "        n_samples_train=n_samples_train,\n",
    "        ax=ax,\n",
    "        fig=fig\n",
    "    )"
   ],
   "id": "46bc2793fe4447fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from compare_variance_residual.variance_partitioning import variance_partitioning",
   "id": "e90d33f76031cd16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "(score_0, score_1, joint_score, shared, x0_unique, x1_unique) = variance_partitioning(Xs_train, Xs_test,\n",
    "                                                                                      Y_train,\n",
    "                                                                                      Y_test, cv=cv,\n",
    "                                                                                      alphas=alphas,\n",
    "                                                                                      logger=logger)\n",
    "vp_errorbar(score_0, score_1, joint_score, shared, x0_unique, x1_unique, feature_space_dimensions, scalars, n_targets,\n",
    "            n_samples_train, n_samples_test, noise_scalar, ax=axs[0], fig=fig)\n",
    "axs[0].set_title(\"Ridge\")\n",
    "(score_0, score_1, joint_score, shared, x0_unique, x1_unique) = variance_partitioning(Xs_train, Xs_test,\n",
    "                                                                                      Y_train,\n",
    "                                                                                      Y_test, cv=cv,\n",
    "                                                                                      alphas=alphas,\n",
    "                                                                                      logger=logger, use_ols=True)\n",
    "vp_errorbar(score_0, score_1, joint_score, shared, x0_unique, x1_unique, feature_space_dimensions, scalars, n_targets,\n",
    "            n_samples_train, n_samples_test, noise_scalar, ax=axs[1], fig=fig)\n",
    "axs[1].set_title(\"OLS\")\n",
    "\n",
    "plt.show()"
   ],
   "id": "b1276f61678c56c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Residual Method",
   "id": "3bbabd3112271167"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from compare_variance_residual.residual import residual_method",
   "id": "1900783e21f6d5af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def rm_errorbar(full_score_0, full_score_1, feature_score_0, feature_score_1, residual_score_0, residual_scores_1,\n",
    "                feature_space_dimensions, scalars,\n",
    "                n_targets, n_samples_train, n_samples_test, noise_scalar, ax: axes.Axes = None, fig=None):\n",
    "    if ax is None or fig is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    # Precompute means and variances\n",
    "    scores = [full_score_0, full_score_1, feature_score_0, feature_score_1, residual_score_0, residual_scores_1]\n",
    "    means = [s.mean() for s in scores]\n",
    "    variances = [s.var() for s in scores]\n",
    "\n",
    "    bar_names = [r\"$X_0$\", r\"$X_1$\", r\"$f(X_1) \\approx X_0$\", r\"$g(X_0) \\approx X_1$\", r\"$X_0 \\setminus X_1$\",\n",
    "                 r\"$X_1 \\setminus X_0$\"]\n",
    "\n",
    "    horizontal_positions = [\n",
    "        (scalars[0] + scalars[1], r\"$a_S + a_{U_0}$\", -0.5, 0.5),\n",
    "        (scalars[0] + scalars[2], r\"$a_S + a_{U_1}$\", 0.5, 1.5),\n",
    "        (feature_space_dimensions[0] / (feature_space_dimensions[0] + feature_space_dimensions[1]),\n",
    "         r\"$\\frac{|S|}{|U_0|+|S|}$\", 1.5, 2.5),\n",
    "        (feature_space_dimensions[0] / (feature_space_dimensions[0] + feature_space_dimensions[2]),\n",
    "         r\"$\\frac{|S|}{|U_1|+|S|}$\", 2.5, 3.5),\n",
    "        (scalars[1], r\"$a_{U_0}$\", 3.5, 4.5),\n",
    "        (scalars[2], r\"$a_{U_1}$\", 4.5, 5.5)\n",
    "    ]\n",
    "\n",
    "    # Call the generic_errorbar function with the required data and parameters\n",
    "    generic_errorbar_plot(\n",
    "        means=means,\n",
    "        variances=variances,\n",
    "        labels=bar_names,\n",
    "        horizontal_positions=horizontal_positions,\n",
    "        title=fr\"{n_targets} targets, $a_S$: {scalars[0]:.2f}, $|S|$: {feature_space_dimensions[0]}, $a_{{U_0}}$: {scalars[1]:.2f}, $|U_0|$: {feature_space_dimensions[1]}, $a_{{U_1}}$: {scalars[2]:.2f}, $|U_1|$: {feature_space_dimensions[2]}, $a_E$: {noise_scalar}\",\n",
    "        suptitle=\"Residual Method\",\n",
    "        ylabel=r\"Average Variance Explained ($r^2$)\",\n",
    "        xlabel=\"Feature space/Model\",\n",
    "        n_samples_test=n_samples_test,\n",
    "        n_samples_train=n_samples_train,\n",
    "        ax=ax,\n",
    "        fig=fig\n",
    "    )"
   ],
   "id": "1c4e4f477d4c7b8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "full_score_0, full_score_1, feature_score_0, feature_score_1, residual_score_0, residual_scores_1 = residual_method(\n",
    "    Xs_train, Xs_test, Y_train, Y_test, return_full_variance=True, cv=cv,\n",
    "    score_func=himalaya.scoring.r2_score\n",
    ")\n",
    "rm_errorbar(full_score_0, full_score_1, feature_score_0, feature_score_1, residual_score_0, residual_scores_1,\n",
    "            feature_space_dimensions, scalars, n_targets, n_samples_train, n_samples_test, noise_scalar, ax=axs[0],\n",
    "            fig=fig)\n",
    "axs[0].set_title(\"Ridge\")\n",
    "full_score_0, full_score_1, feature_score_0, feature_score_1, residual_score_0, residual_scores_1 = residual_method(\n",
    "    Xs_train, Xs_test, Y_train, Y_test, use_ols=True, return_full_variance=True, cv=cv,\n",
    "    score_func=himalaya.scoring.r2_score\n",
    ")\n",
    "rm_errorbar(full_score_0, full_score_1, feature_score_0, feature_score_1, residual_score_0, residual_scores_1,\n",
    "            feature_space_dimensions, scalars, n_targets, n_samples_train, n_samples_test, noise_scalar, ax=axs[1],\n",
    "            fig=fig)\n",
    "axs[1].set_title(\"OLS\")\n",
    "\n",
    "plt.show()"
   ],
   "id": "aa6438f4ddd7127a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dimensions",
   "id": "705090ca62e5b43a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(12, 12), sharey='row')\n",
    "\n",
    "for i, shared_dimension in enumerate([10, 100, 1000]):\n",
    "    dimensions = feature_space_dimensions.copy()\n",
    "    dimensions[0] = shared_dimension\n",
    "\n",
    "    _n_samples_train = 2 * sum(dimensions)\n",
    "\n",
    "    (Xs_train, Xs_test, Y_train, Y_test) = generate_dataset(dimensions, scalars, n_targets,\n",
    "                                                            _n_samples_train,\n",
    "                                                            n_samples_test, noise_scalar)\n",
    "    (score_0, score_1, joint_score, shared, x0_unique, x1_unique) = variance_partitioning(Xs_train, Xs_test,\n",
    "                                                                                          Y_train,\n",
    "                                                                                          Y_test, cv=cv)\n",
    "    vp_errorbar(score_0, score_1, joint_score, shared, x0_unique, x1_unique, dimensions, scalars, n_targets,\n",
    "                _n_samples_train, n_samples_test, noise_scalar, ax=axs[i, 0], fig=fig)\n",
    "    axs[i, 0].set_title(f\"Variance Partitioning (Shared Dim: {shared_dimension})\")\n",
    "\n",
    "    (full_score_0, full_score_1, feature_score_0, feature_score_1, residual_score_0,\n",
    "     residual_scores_1) = residual_method(\n",
    "        Xs_train, Xs_test, Y_train, Y_test, use_ols=True, return_full_variance=True, cv=cv)\n",
    "\n",
    "    rm_errorbar(full_score_0, full_score_1, feature_score_0, feature_score_1, residual_score_0, residual_scores_1,\n",
    "                dimensions, scalars, n_targets, _n_samples_train, n_samples_test, noise_scalar, ax=axs[i, 1], fig=fig)\n",
    "    axs[i, 1].set_title(f\"Residual Method (Shared Dim: {shared_dimension})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "4c495c37ab297e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Scalars",
   "id": "d51ef40e29ec0a24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Updated code to put each plot into the subplot axes, with scalars along the rows and vp and rm in the columns\n",
    "n_scalars = 5\n",
    "fig, axs = plt.subplots(n_scalars, 2, figsize=(12, n_scalars * 4), sharey='row')  # Adjust figure size dynamically\n",
    "\n",
    "for i, shared_scalar in enumerate(np.linspace(0, 1, n_scalars)):\n",
    "    _scalars = [shared_scalar, (1 - shared_scalar) / 2, (1 - shared_scalar) / 2]\n",
    "    (Xs_train, Xs_test, Y_train, Y_test) = generate_dataset(feature_space_dimensions, _scalars,\n",
    "                                                            n_targets,\n",
    "                                                            n_samples_train, n_samples_test,\n",
    "                                                            noise_scalar)\n",
    "    (score_0, score_1, joint_score, shared, x0_unique, x1_unique) = variance_partitioning(Xs_train, Xs_test,\n",
    "                                                                                          Y_train,\n",
    "                                                                                          Y_test, cv=cv,\n",
    "                                                                                          score_func=himalaya.scoring.r2_score)\n",
    "    # Variance Partitioning subplot\n",
    "    vp_errorbar(score_0, score_1, joint_score, shared, x0_unique, x1_unique, feature_space_dimensions, _scalars,\n",
    "                n_targets, n_samples_train, n_samples_test, noise_scalar, ax=axs[i, 0], fig=fig)\n",
    "    axs[i, 0].set_title(f\"Variance Partitioning (Shared scalar: {shared_scalar:.2f})\")\n",
    "\n",
    "    # Residual Method subplot\n",
    "    full_score_0, full_score_1, feature_score_0, feature_score_1, residual_score_0, residual_scores_1 = residual_method(\n",
    "        Xs_train, Xs_test, Y_train, Y_test, use_ols=True, return_full_variance=True, cv=cv,\n",
    "        score_func=himalaya.scoring.r2_score)\n",
    "\n",
    "    rm_errorbar(full_score_0, full_score_1, feature_score_0, feature_score_1, residual_score_0, residual_scores_1,\n",
    "                feature_space_dimensions, _scalars, n_targets, n_samples_train, n_samples_test, noise_scalar,\n",
    "                ax=axs[i, 1], fig=fig)\n",
    "    axs[i, 1].set_title(f\"Residual Method (Shared scalar: {shared_scalar:.2f})\")\n",
    "\n",
    "plt.tight_layout()  # To avoid overlap of subplots\n",
    "plt.show()"
   ],
   "id": "b61c6482cc2e8931"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "afbf506a05b756ae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
