{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:31:45.639058Z",
     "start_time": "2025-02-20T15:31:45.636486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import simplstyles\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from himalaya.backend import set_backend\n",
    "\n",
    "from compare_variance_residual.residual import residual_method\n",
    "from compare_variance_residual.simulation import generate_dataset\n",
    "from compare_variance_residual.variance_partitioning import variance_partitioning"
   ],
   "id": "ebc7e983ef1a6851",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:31:45.683354Z",
     "start_time": "2025-02-20T15:31:45.680829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_path(alphas, cv, n_targets):\n",
    "    path = os.path.join(\"results\", f\"targets={n_targets}\", f\"cv={cv}\",\n",
    "                        f\"alphas={alphas.min()},{alphas.max()},{len(alphas)}\", \"varying training samples\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path"
   ],
   "id": "5aa82aa99fe1c34d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:31:45.733014Z",
     "start_time": "2025-02-20T15:31:45.727771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_scores(samples_train, d_list, scalars, n_targets, n_samples_test, noise_target, cv, alphas):\n",
    "    path = get_path(alphas, cv, n_targets)\n",
    "    for n_samples_train in samples_train:\n",
    "        print(n_samples_train)\n",
    "        csv_path = os.path.join(path, f\"scores_{n_samples_train}.csv\")\n",
    "        scores = pd.DataFrame()\n",
    "        if os.path.exists(csv_path):\n",
    "            print(\"skipping, already exists\")\n",
    "            continue\n",
    "        Xs, Y = generate_dataset(d_list, scalars, n_targets, n_samples_train + n_samples_test, noise_target)\n",
    "        print(\"data generated\")\n",
    "        x1_score, x2_score, joint_score, x1_and_x2_score, vp_x1_unique_score, vp_x2_unique_score = variance_partitioning(\n",
    "            Xs, Y, n_samples_train, alphas, cv)\n",
    "        print(\"variance partitioning done\")\n",
    "\n",
    "        scores[\"x1_score\"] = x1_score\n",
    "        scores[\"x2_score\"] = x2_score\n",
    "        scores[\"vp_joint_score\"] = joint_score\n",
    "        scores[\"vp_shared_score\"] = x1_and_x2_score\n",
    "        scores[\"vp_x1_unique_score\"] = vp_x1_unique_score\n",
    "        scores[\"vp_x2_unique_score\"] = vp_x2_unique_score\n",
    "        del x1_score, x2_score, joint_score, x1_and_x2_score, vp_x1_unique_score, vp_x2_unique_score\n",
    "\n",
    "        _, _, x2_to_x1_score, x1_to_x2_score, rm_x1_unique_score, rm_x2_unique_score = residual_method(\n",
    "            Xs, Y, n_samples_train, alphas, cv)\n",
    "        print(\"residual method done\")\n",
    "        scores[\"rm_x2_to_x1_score\"] = np.concatenate(\n",
    "            [x2_to_x1_score, np.full(len(rm_x1_unique_score) - len(x2_to_x1_score), np.nan)])\n",
    "        scores[\"rm_x1_to_x2_score\"] = np.concatenate(\n",
    "            [x1_to_x2_score, np.full(len(rm_x1_unique_score) - len(x1_to_x2_score), np.nan)])\n",
    "        scores[\"rm_x1_unique_score\"] = rm_x1_unique_score\n",
    "        scores[\"rm_x2_unique_score\"] = rm_x2_unique_score\n",
    "        print(scores.head())\n",
    "        del x2_to_x1_score, x1_to_x2_score, rm_x1_unique_score, rm_x2_unique_score\n",
    "        del Xs, Y\n",
    "        scores.to_csv(csv_path, index=False)"
   ],
   "id": "31a2f3c4bc89faa3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save scores for varying training samples",
   "id": "1b3968d2b74cef95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:31:45.777145Z",
     "start_time": "2025-02-20T15:31:45.774400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "backend = set_backend(\"cupy\", on_error=\"warn\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(42)\n",
    "plt.style.use('nord-light-talk')"
   ],
   "id": "5db6a3f001bc68a7",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:31:45.824122Z",
     "start_time": "2025-02-20T15:31:45.821168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_list = [100, 100, 100]\n",
    "n_targets = 10000\n",
    "n_samples_test = 100\n",
    "scalars = [1 / 3, 1 / 3, 1 / 3]\n",
    "noise_target = 0.1\n",
    "\n",
    "cv = 10\n",
    "alphas = np.logspace(-5, 5, 10)"
   ],
   "id": "d189793703ded5b2",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:31:45.871684Z",
     "start_time": "2025-02-20T15:31:45.868397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "samples_train = np.logspace(3, 4.47712125472, 5).astype(int)\n",
    "samples_train"
   ],
   "id": "d49b37e7ca289ac0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1000,  2340,  5477, 12818, 30000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-20T15:31:45.916279Z"
    }
   },
   "cell_type": "code",
   "source": "save_scores(samples_train, d_list, scalars, n_targets, n_samples_test, noise_target, cv, alphas)",
   "id": "1670fb6475b4f48a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "skipping, already exists\n",
      "2340\n",
      "skipping, already exists\n",
      "5477\n",
      "skipping, already exists\n",
      "12818\n",
      "skipping, already exists\n",
      "30000\n",
      "data generated\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load scores",
   "id": "3e5708b3b6f87bda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vp = pd.DataFrame()\n",
    "rm = pd.DataFrame()\n",
    "\n",
    "for i, n_samples_train in enumerate(samples_train):\n",
    "    scores = pd.read_csv(os.path.join(get_path(alphas, cv, n_targets), f\"scores_{n_samples_train}.csv\"))\n",
    "    vp_x1_unique_predicted = scores['vp_x1_unique_score']\n",
    "    vp_x1_se = (vp_x1_unique_predicted - scalars[1]).std()\n",
    "    vp_scores = pd.DataFrame({\n",
    "        'n_samples_train': n_samples_train,\n",
    "        'vp_joint_score': scores['vp_joint_score'].mean(),\n",
    "        'vp_x1_unique_score': vp_x1_unique_predicted.mean(),\n",
    "        'vp_x1_se': vp_x1_se,\n",
    "        'vp_x1_lower_se': vp_x1_unique_predicted.mean() - vp_x1_se,\n",
    "        'vp_x1_upper_se': vp_x1_unique_predicted.mean() + vp_x1_se\n",
    "    }, index=[i])\n",
    "\n",
    "    rm_x1_unique_score = scores['rm_x1_unique_score']\n",
    "    rm_x1_se = (rm_x1_unique_score - scalars[1]).std()\n",
    "    rm_scores = pd.DataFrame({\n",
    "        'n_samples_train': n_samples_train,\n",
    "        'rm_x1_unique_score': rm_x1_unique_score.mean(),\n",
    "        'rm_x1_se': rm_x1_se,\n",
    "        'rm_x1_lower_se': rm_x1_unique_score.mean() - rm_x1_se,\n",
    "        'rm_x1_upper_se': rm_x1_unique_score.mean() + rm_x1_se\n",
    "    }, index=[i])\n",
    "\n",
    "    vp = pd.concat([vp, vp_scores], ignore_index=True)\n",
    "    rm = pd.concat([rm, rm_scores], ignore_index=True)\n",
    "vp.head()\n",
    "rm.head()"
   ],
   "id": "ab17805fe3f177ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "hue_order = ['Residual Method', 'Variance Partitioning', ]\n",
    "sns.lineplot(data=vp, x='n_samples_train', y='vp_x1_unique_score', label='Variance Partitioning', palette='C2')\n",
    "plt.fill_between(vp['n_samples_train'], vp['vp_x1_lower_se'], vp['vp_x1_upper_se'], alpha=0.3)\n",
    "sns.lineplot(data=rm, x='n_samples_train', y='rm_x1_unique_score', label='Residual Method', hue_order=hue_order)\n",
    "plt.fill_between(rm['n_samples_train'], rm['rm_x1_lower_se'], rm['rm_x1_upper_se'], alpha=0.3)\n",
    "plt.axhline(scalars[1], linestyle='--', label='true unique variance')\n",
    "plt.xlabel(r\"Numer of Training Samples\")\n",
    "plt.ylabel(r\"Predicted Unique Variance (avg. $R^2$)\")\n",
    "plt.legend()"
   ],
   "id": "af4e16570e910b46"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
