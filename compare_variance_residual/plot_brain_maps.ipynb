{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T08:45:45.412179Z",
     "start_time": "2025-02-23T08:45:45.030296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import simplstyles\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import voxelwise_tutorials.viz as viz\n",
    "from himalaya.backend import set_backend\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from compare_variance_residual.stimuli_utils.TemporalChunkSplitter import TemporalChunkSplitter"
   ],
   "id": "b5dc9c4e71fd8255",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T20:52:05.198437Z",
     "start_time": "2025-02-22T20:52:05.195898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_result_path(modality, subject):\n",
    "    path = os.path.join(\"results\", modality, f\"subject{subject:02}\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path"
   ],
   "id": "e974193db6e9033a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T08:45:53.108413Z",
     "start_time": "2025-02-23T08:45:51.307180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.style.use('nord-light-talk')\n",
    "data_dir = \"../data\"\n",
    "backend = set_backend('torch_cuda', on_error='warn')"
   ],
   "id": "4865fc9191590ff5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T20:52:06.574813Z",
     "start_time": "2025-02-22T20:52:06.572469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "language_model = \"bert-base\"\n",
    "layer = 9\n",
    "num_layers = 12\n",
    "feature = \"semantic\"\n",
    "modality = \"reading\"\n",
    "subject = 1\n",
    "low_level_feature = \"letters\"\n",
    "trim = 5  # remove 5 TRs from the start and end of each story\n",
    "sequence_length = 20\n",
    "number_of_delays = 4"
   ],
   "id": "75f6b6cc5a72a947",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T20:52:06.621246Z",
     "start_time": "2025-02-22T20:52:06.618909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "alphas = np.logspace(-5, 5, 10)\n",
    "cv = 10\n",
    "n_iter = 10"
   ],
   "id": "6fa8a056a5f60f05",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Features",
   "id": "9a8914d9d0cdb926"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T08:46:17.671190Z",
     "start_time": "2025-02-23T08:46:17.667045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import h5py\n",
    "\n",
    "features_train = h5py.File(os.path.join(data_dir, 'features', 'features_trn_NEW.hdf'), 'r')\n",
    "features_val = h5py.File(os.path.join(data_dir, 'features', 'features_val_NEW.hdf'), 'r')\n",
    "print(features_train.keys(), features_val.keys())\n",
    "print(features_train['story_01'].keys())"
   ],
   "id": "ae581730bce86b7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['story_01', 'story_02', 'story_03', 'story_04', 'story_05', 'story_06', 'story_07', 'story_08', 'story_09', 'story_10']> <KeysViewHDF5 ['story_11']>\n",
      "<KeysViewHDF5 ['english1000', 'letters', 'numletters', 'numphonemes', 'numwords', 'pauses', 'phonemes', 'word_length_std']>\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Low Level Feature",
   "id": "d0cd231fefc1774f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T20:52:06.731746Z",
     "start_time": "2025-02-22T20:52:06.720078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "low_level_train = np.vstack([zscore(features_train[story][low_level_feature]) for story in features_train.keys()])\n",
    "low_level_val = np.vstack([zscore(features_val[story][low_level_feature]) for story in features_val.keys()])\n",
    "low_level_train, low_level_val = np.nan_to_num(low_level_train), np.nan_to_num(low_level_val)\n",
    "print(low_level_train.shape, low_level_val.shape)"
   ],
   "id": "dbc1a7e7f96a875a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3887, 26) (306, 26)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load High Level (NLP) Features",
   "id": "5ba918368c5f7de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T20:52:06.925545Z",
     "start_time": "2025-02-22T20:52:06.767686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# downsampled_embeddings = np.load(f\"../{language_model}{sequence_length}_downsampled.npy\", allow_pickle=True)\n",
    "# print(downsampled_embeddings.item().keys())"
   ],
   "id": "506e1dec58eb16d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['alternateithicatom', 'avatar', 'howtodraw', 'legacy', 'life', 'myfirstdaywiththeyankees', 'naked', 'odetostepfather', 'souls', 'undertheinfluence', 'wheretheressmoke'])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T20:52:07.034941Z",
     "start_time": "2025-02-22T20:52:06.961526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Rstories = ['alternateithicatom', 'avatar', 'howtodraw', 'legacy',\n",
    "#             'life', 'myfirstdaywiththeyankees', 'naked',\n",
    "#             'odetostepfather', 'souls', 'undertheinfluence']\n",
    "# Pstories = ['wheretheressmoke']\n",
    "#\n",
    "# semantic_embeddings_train = np.vstack([zscore(downsampled_embeddings.item()[story][layer]) for story in Rstories])\n",
    "# semantic_embeddings_val = np.vstack([zscore(downsampled_embeddings.item()[story][layer]) for story in Pstories])\n",
    "# semantic_embeddings_train, semantic_embeddings_val = np.nan_to_num(semantic_embeddings_train), np.nan_to_num(\n",
    "#     semantic_embeddings_val)\n",
    "# print(semantic_embeddings_train.shape, semantic_embeddings_val.shape)"
   ],
   "id": "3d85bea9cacd735b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3887, 768) (306, 768)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T08:46:31.823451Z",
     "start_time": "2025-02-23T08:46:31.768944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "semantic_train = np.vstack([zscore(features_train[story]['english1000']) for story in features_train.keys()])\n",
    "semantic_val = np.vstack([zscore(features_val[story]['english1000']) for story in features_val.keys()])\n",
    "print(semantic_train.shape, semantic_val.shape)"
   ],
   "id": "160066a1ea570c1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3887, 985) (306, 985)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Brain Data",
   "id": "21566d9ac985f3c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T20:52:14.296602Z",
     "start_time": "2025-02-22T20:52:07.071021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from voxelwise_tutorials.io import load_hdf5_array\n",
    "\n",
    "Y_train_filename = os.path.join(data_dir, 'responses', f'subject{subject:02}_{modality}_fmri_data_trn.hdf')\n",
    "Y_train = load_hdf5_array(Y_train_filename)\n",
    "\n",
    "Y_test_filename = os.path.join(data_dir, 'responses', f'subject{subject:02}_{modality}_fmri_data_val.hdf')\n",
    "Y_test = load_hdf5_array(Y_test_filename)\n",
    "\n",
    "Y_train = np.vstack([zscore(Y_train[story][:-trim]) for story in Y_train.keys()])\n",
    "Ys_test = [np.vstack([zscore(Y_test[story][i][:-trim]) for story in Y_test.keys()]) for i in range(2)]\n",
    "\n",
    "print(Y_train.shape, np.array(Ys_test).shape)\n",
    "Y_train, Ys_test = np.nan_to_num(Y_train), np.nan_to_num(Ys_test)"
   ],
   "id": "c03613f2231ff7e4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62600/893258922.py:9: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  Y_train = np.vstack([zscore(Y_train[story][:-trim]) for story in Y_train.keys()])\n",
      "/tmp/ipykernel_62600/893258922.py:9: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  Y_train = np.vstack([zscore(Y_train[story][:-trim]) for story in Y_train.keys()])\n",
      "/tmp/ipykernel_62600/893258922.py:9: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  Y_train = np.vstack([zscore(Y_train[story][:-trim]) for story in Y_train.keys()])\n",
      "/tmp/ipykernel_62600/893258922.py:9: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  Y_train = np.vstack([zscore(Y_train[story][:-trim]) for story in Y_train.keys()])\n",
      "/tmp/ipykernel_62600/893258922.py:9: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  Y_train = np.vstack([zscore(Y_train[story][:-trim]) for story in Y_train.keys()])\n",
      "/tmp/ipykernel_62600/893258922.py:9: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  Y_train = np.vstack([zscore(Y_train[story][:-trim]) for story in Y_train.keys()])\n",
      "/tmp/ipykernel_62600/893258922.py:9: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  Y_train = np.vstack([zscore(Y_train[story][:-trim]) for story in Y_train.keys()])\n",
      "/tmp/ipykernel_62600/893258922.py:9: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  Y_train = np.vstack([zscore(Y_train[story][:-trim]) for story in Y_train.keys()])\n",
      "/tmp/ipykernel_62600/893258922.py:9: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  Y_train = np.vstack([zscore(Y_train[story][:-trim]) for story in Y_train.keys()])\n",
      "/tmp/ipykernel_62600/893258922.py:9: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  Y_train = np.vstack([zscore(Y_train[story][:-trim]) for story in Y_train.keys()])\n",
      "/tmp/ipykernel_62600/893258922.py:10: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  Ys_test = [np.vstack([zscore(Y_test[story][i][:-trim]) for story in Y_test.keys()]) for i in range(2)]\n",
      "/tmp/ipykernel_62600/893258922.py:10: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  Ys_test = [np.vstack([zscore(Y_test[story][i][:-trim]) for story in Y_test.keys()]) for i in range(2)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3887, 81133) (2, 306, 81133)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## convert everything to float32",
   "id": "ca467a3ad4040c09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T20:52:14.647935Z",
     "start_time": "2025-02-22T20:52:14.333784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "low_level_train = low_level_train.astype(np.float32)\n",
    "low_level_val = low_level_val.astype(np.float32)\n",
    "semantic_train = semantic_train.astype(np.float32)\n",
    "semantic_val = semantic_val.astype(np.float32)\n",
    "Y_train = Y_train.astype(np.float32)\n",
    "Ys_test = [Y_test.astype(np.float32) for Y_test in Ys_test]"
   ],
   "id": "a1e9de7aa773056f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Variance Partitioning",
   "id": "3451b505f2a885bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Low Level Prediction",
   "id": "e2d6bc50614f533f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T20:52:14.687921Z",
     "start_time": "2025-02-22T20:52:14.685735Z"
    }
   },
   "cell_type": "code",
   "source": "low_level_file = os.path.join(get_result_path(modality, subject), f\"vp_low_level_{low_level_feature}_scores.csv\")",
   "id": "a1dfaf45fce88d69",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T20:52:14.764129Z",
     "start_time": "2025-02-22T20:52:14.732983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from himalaya.ridge import RidgeCV\n",
    "from voxelwise_tutorials.delayer import Delayer\n",
    "\n",
    "if not os.path.exists(low_level_file):\n",
    "    print(f\"Saving {low_level_file}\")\n",
    "    # delay stimuli to account for hemodynamic lag\n",
    "    delays = range(1, number_of_delays + 1)\n",
    "    delayer = Delayer(delays=delays)\n",
    "    pipeline = make_pipeline(\n",
    "        delayer,\n",
    "        RidgeCV(\n",
    "            alphas=alphas, cv=cv,\n",
    "            solver_params=dict(n_targets_batch=50, n_alphas_batch=1, n_targets_batch_refit=50)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    pipeline.fit(low_level_train, Y_train)\n",
    "    vp_low_level_scores = []\n",
    "    for Y_test in Ys_test:\n",
    "        vp_low_level_scores.append(pipeline.score(low_level_val, Y_test))\n",
    "    vp_low_level_scores = np.array(vp_low_level_scores)\n",
    "    # save as csv\n",
    "    vp_low_level_scores = pd.DataFrame(vp_low_level_scores)\n",
    "    vp_low_level_scores.to_csv(low_level_file)\n",
    "else:\n",
    "    print(f\"Loading {low_level_file}\")\n",
    "    vp_low_level_scores = pd.read_csv(low_level_file, index_col=0).values\n",
    "print(vp_low_level_scores.max(), vp_low_level_scores.min(), vp_low_level_scores.mean())"
   ],
   "id": "138a40d0f43fa082",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results/reading/subject01/vp_low_level_letters_scores.csv\n",
      "0.3245427479184268 -0.0631824860921701 0.0023216666772059953\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Semantic Prediction",
   "id": "826c08588e853c04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T20:52:14.806271Z",
     "start_time": "2025-02-22T20:52:14.804251Z"
    }
   },
   "cell_type": "code",
   "source": "semantic_file = os.path.join(get_result_path(modality, subject), f\"vp_semantic_{layer:02}_scores.csv\")",
   "id": "df7bc2b066892f3d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T20:52:14.877280Z",
     "start_time": "2025-02-22T20:52:14.850438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from himalaya.ridge import RidgeCV\n",
    "from voxelwise_tutorials.delayer import Delayer\n",
    "\n",
    "if not os.path.exists(semantic_file):\n",
    "    print(f\"Saving {semantic_file}\")\n",
    "    # delay stimuli to account for hemodynamic lag\n",
    "    delays = range(1, number_of_delays + 1)\n",
    "    delayer = Delayer(delays=delays)\n",
    "    pipeline = make_pipeline(\n",
    "        delayer,\n",
    "        RidgeCV(\n",
    "            alphas=alphas, cv=cv,\n",
    "            solver_params=dict(n_targets_batch=50, n_alphas_batch=1, n_targets_batch_refit=50)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    pipeline.fit(semantic_train, Y_train)\n",
    "    vp_semantic_scores = []\n",
    "    for Y_test in Ys_test:\n",
    "        vp_semantic_scores.append(pipeline.score(semantic_val, Y_test))\n",
    "    # save as csv\n",
    "    vp_semantic_scores = pd.DataFrame(\n",
    "        {'vp_semantic_score_0': vp_semantic_scores[0], 'vp_semantic_score_1': vp_semantic_scores[1]})\n",
    "    vp_semantic_scores.to_csv(semantic_file)\n",
    "else:\n",
    "    print(f\"Loading {semantic_file}\")\n",
    "    vp_semantic_scores = pd.read_csv(semantic_file, index_col=0).values\n",
    "print(vp_semantic_scores.max(), vp_semantic_scores.min(), vp_semantic_scores.mean())"
   ],
   "id": "93c3deb381460802",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results/reading/subject01/vp_semantic_09_scores.csv\n",
      "0.3457906230440486 -0.1617008729669207 0.0023683850304021454\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Joint Prediction",
   "id": "14b7f17e56130ec2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T20:52:14.920466Z",
     "start_time": "2025-02-22T20:52:14.918433Z"
    }
   },
   "cell_type": "code",
   "source": "joint_file = os.path.join(get_result_path(modality, subject), f\"vp_joint_{feature}_{low_level_feature}_scores.csv\")",
   "id": "8b67cb6f4d181f54",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T21:10:08.382926Z",
     "start_time": "2025-02-22T21:10:08.358126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from himalaya.ridge import BandedRidgeCV, ColumnTransformerNoStack\n",
    "\n",
    "if not os.path.exists(joint_file):\n",
    "    print(f\"Saving {joint_file}\")\n",
    "    # delay stimuli to account for hemodynamic lag\n",
    "    delays = range(1, number_of_delays + 1)\n",
    "    delayer = Delayer(delays=delays)\n",
    "\n",
    "    start_and_end = np.concatenate([[0], np.cumsum([semantic_train.shape[1], low_level_train.shape[1]])])\n",
    "    slices = [\n",
    "        slice(start, end)\n",
    "        for start, end in zip(start_and_end[:-1], start_and_end[1:])\n",
    "    ]\n",
    "    print(slices)\n",
    "    ct = ColumnTransformerNoStack(transformers=[('semantic', delayer, slices[0]), ('low_level', delayer, slices[1])])\n",
    "\n",
    "    pipeline = make_pipeline(\n",
    "        ct,\n",
    "        BandedRidgeCV(\n",
    "            cv=TemporalChunkSplitter(), groups=\"input\",\n",
    "            solver_params=dict(n_iter=n_iter, alphas=alphas, n_targets_batch=500, n_alphas_batch=5,\n",
    "                               n_targets_batch_refit=100)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    X_train = np.concatenate([semantic_train, low_level_train], axis=1)\n",
    "    X_test = np.concatenate([semantic_val, low_level_val], axis=1)\n",
    "    pipeline.fit(X_train, Y_train)\n",
    "\n",
    "    vp_joint_scores = []\n",
    "    for Y_test in Ys_test:\n",
    "        vp_joint_scores.append(pipeline.score(X_test, Y_test))\n",
    "    vp_joint_scores = vp_joint_scores.cpu()\n",
    "    # save as csv\n",
    "    vp_joint_scores = pd.DataFrame({'vp_joint_score_0': vp_joint_scores[0], 'vp_joint_score_1': vp_joint_scores[1]})\n",
    "    vp_joint_scores.to_csv(joint_file)\n",
    "else:\n",
    "    print(f\"Loading {joint_file}\")\n",
    "    vp_joint_scores = pd.read_csv(joint_file, index_col=0).values\n",
    "print(vp_joint_scores.max(), vp_joint_scores.min(), vp_joint_scores.mean())"
   ],
   "id": "a98b775b1dd1f680",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results/reading/subject01/vp_joint_semantic_letters_scores.csv\n",
      "0.3585633 -0.11069548 0.004417512358557558\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Residual Method",
   "id": "4c7615c170ab5a2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T21:09:40.689609Z",
     "start_time": "2025-02-22T21:09:40.473861Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2c51f150e6695d95",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plot Brain Maps",
   "id": "954bdd603b46c57d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_flatmap_from_mapper(voxels, mapper_file, ax=None, alpha=0.7, cmap=plt.get_cmap(), vmin=None, vmax=None,\n",
    "                             with_curvature=True, with_rois=True, with_colorbar=True,\n",
    "                             colorbar_location=(.4, .9, .2, .05)):\n",
    "    \"\"\"Plot a flatmap from a mapper file, with 1D data.\n",
    "\n",
    "    This function is equivalent to the pycortex functions:\n",
    "    cortex.quickshow(cortex.Volume(voxels, ...), ...)\n",
    "\n",
    "    Note that this function does not have the full capability of pycortex,\n",
    "    since it is based on flatmap mappers and not on the original brain\n",
    "    surface of the subject.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    voxels : array of shape (n_voxels, )\n",
    "        Data to be plotted.\n",
    "    mapper_file : str\n",
    "        File name of the mapper.\n",
    "    ax : matplotlib Axes or None.\n",
    "        Axes where the figure will be plotted.\n",
    "        If None, a new figure is created.\n",
    "    alpha : float in [0, 1], or array of shape (n_voxels, )\n",
    "        Transparency of the flatmap.\n",
    "    cmap : str\n",
    "        Name of the matplotlib colormap.\n",
    "    vmin : float or None\n",
    "        Minimum value of the colormap. If None, use the 1st percentile of the\n",
    "        `voxels` array.\n",
    "    vmax : float or None\n",
    "        Minimum value of the colormap. If None, use the 99th percentile of the\n",
    "        `voxels` array.\n",
    "    with_curvature : bool\n",
    "        If True, show the curvature below the data layer.\n",
    "    with_rois : bool\n",
    "        If True, show the ROIs labels above the data layer.\n",
    "    colorbar_location : [left, bottom, width, height]\n",
    "        Location of the colorbar. All quantities are in fractions of figure\n",
    "        width and height.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ax : matplotlib Axes\n",
    "        Axes where the figure has been plotted.\n",
    "    \"\"\"\n",
    "    # create a figure\n",
    "    if ax is None:\n",
    "        flatmap_mask = load_hdf5_array(mapper_file, key='flatmap_mask')\n",
    "        figsize = np.array(flatmap_mask.shape) / 100.\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.add_axes((0, 0, 1, 1))\n",
    "        ax.axis('off')\n",
    "\n",
    "    # process plotting parameters\n",
    "    vmin = np.percentile(voxels, 1) if vmin is None else vmin\n",
    "    vmax = np.percentile(voxels, 99) if vmax is None else vmax\n",
    "    if isinstance(alpha, np.ndarray):\n",
    "        alpha = viz.map_voxels_to_flatmap(alpha, mapper_file)\n",
    "\n",
    "    # plot the data\n",
    "    image = viz.map_voxels_to_flatmap(voxels, mapper_file)\n",
    "    cimg = ax.imshow(image, aspect='equal', zorder=1, alpha=alpha, cmap=cmap,\n",
    "                     vmin=vmin, vmax=vmax)\n",
    "\n",
    "    if with_colorbar:\n",
    "        try:\n",
    "            cbar = ax.inset_axes(colorbar_location)\n",
    "        except AttributeError:  # for matplotlib < 3.0\n",
    "            cbar = ax.figure.add_axes(colorbar_location)\n",
    "        colorbar = ax.figure.colorbar(cimg, cax=cbar, orientation='horizontal')\n",
    "        colorbar.ax.set_title(\"Pearson Correlation of Y_true and Y_pred\", fontsize=14)\n",
    "\n",
    "    # plot additional layers if present\n",
    "    viz._plot_addition_layers(ax=ax, n_voxels=voxels.shape[0],\n",
    "                              mapper_file=mapper_file,\n",
    "                              with_curvature=with_curvature, with_rois=with_rois)\n",
    "\n",
    "    return ax"
   ],
   "id": "31138f1d9ccc8738",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path = get_path(language_model, feature, modality, subject, low_level_feature, layer)\n",
    "correlation = np.nan_to_num(np.load(path, allow_pickle=True))\n",
    "mapper_path = os.path.join(\"../data\", 'mappers', f\"subject{subject:02}_mappers.hdf\")\n",
    "flatmap_mask = load_hdf5_array(mapper_path, key='flatmap_mask')"
   ],
   "id": "e415d17fbe50326e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "figsize = np.array(flatmap_mask.shape) / 100.\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes((0, 0, 1, 1))\n",
    "ax.axis('off')\n",
    "plot_flatmap_from_mapper(correlation, mapper_path, ax=ax, with_curvature=False, alpha=1, vmin=0,\n",
    "                         # vmin=np.min(correlation),\n",
    "                         vmax=np.max(correlation), colorbar_location=[0.75, 0.05, 0.2, 0.05])\n",
    "plt.show()"
   ],
   "id": "8f59402dbd0c7cd1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
